{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 자료형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch의 텐서(Tensor) 자료형은 NumPy의 배열(ndarray)과 유사한 자료형이다. 텐서 자료형을 사용하는 방법도 NumPy와 비슷하다. 텐서 자료형 데이터를 만드는 방법은 3 가지가 있다.\n",
    "\n",
    "+ 리스트나 NumPy 배열을 텐서로 변환\n",
    "+ 0 또는 1 등의 특정한 값을 가진 텐서를 생성\n",
    "+ 랜덤한 값을 가지는 텐서를 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 배열과 리스트를 텐서 자료형으로 변환\n",
    "\n",
    "리스트를 텐서 자료형으로 바꾸러면 **torch.tensor( )** 또는 **torch.as_tensor( )**, **torch.from_numpy( )** 명령을 사용한다.\n",
    "\n",
    "* torch.tensor( ) : 값 복사(value copy)를 사용하여 새로운 텐서 자료형 인스턴스를 생성한다.\n",
    "* torch.as_tensor( ): 리스트나 ndarray 객체를 받는다. 값 참조(refernce)를 사용하여 텐서 자료형 뷰(view)를 만든다.\n",
    "* torch.from_numpy( ) : ndarray 객체를 받는다. 값 참조(refernce)를 사용하여 텐서 자료형 뷰(view)를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "li = np.array([[1, 2], [3, 4]])\n",
    "li_tensor = torch.tensor(li)\n",
    "li_as_tensor = torch.as_tensor(li)\n",
    "\n",
    "print(li_tensor)\n",
    "print(li_as_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.tensor() 명령으로 텐서를 만들거나 torch.as_tensor()에 ndarray가 아닌 리스트를 넣었을 때는 이러한 일이 발생하지 않는다.\n",
    "\n",
    "NumPy 배열을 텐서로 바꿀 때는 **torch.from_numpy( )** 함수를 쓸 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]]) torch.int64\n",
      "tensor([[1, 2],\n",
      "        [3, 4]]) torch.int64\n",
      "tensor([[1, 2],\n",
      "        [3, 4]]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "arr_tensor = torch.tensor(arr)\n",
    "arr_as_tensor = torch.as_tensor(arr)\n",
    "arr_from_numpy = torch.from_numpy(arr)\n",
    "\n",
    "print(arr_tensor, arr_tensor.dtype)\n",
    "print(arr_as_tensor, arr_as_tensor.dtype)\n",
    "print(arr_from_numpy, arr_from_numpy.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반대로 텐서를 NumPy 배열로 바꿀 때는 **torch.numpy( )** 메서드를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "print(arr_tensor.numpy())\n",
    "print(arr_as_tensor.numpy())\n",
    "print(arr_from_numpy.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.as_tensor( )나 torch.from_numpy( )는 원래의 ndarray 객체를 참조하므로 **ndarray 객체의 값을 바꾸면 텐서 자료형의 값도 바뀌고 반대로 텐서 자료형에서 원소의 값을 바꾸면 원래 ndarray 객체의 값도 바뀐다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1000,    2],\n",
       "       [   3,    4]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_as_tensor[0, 0] = 1000\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1000,    2],\n",
       "        [   3,    4]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_from_numpy # 같은 객체를 참조한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1000, 2000],\n",
       "        [   3,    4]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[0, 1] = 2000\n",
    "arr_as_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 랜덤한 값을 가지는 텐서 생성\n",
    "\n",
    "랜덤 값으로 채워진 텐서를 생성하는 명령은 다음과 같다.\n",
    "\n",
    "* torch.rand( ) : 0과 1사이의 숫자를 균등하게 생성\n",
    "* torch.rand_like( ) : 사이즈를 튜플로 입력하지 않고 기존의 텐서로 정의\n",
    "* torch.randn( ) : 평균이 0이고 표준편차가 1인 가우시안 정규분포를 이용해 생성\n",
    "* torch.randn_like( ) : 사이즈를 튜플로 입력하지 않고 기존의 텐서로 정의\n",
    "* torch.randint( ) : 주어진 범위 내의 정수를 균등하게 생성, 자료형은 torch.float32\n",
    "* torch.randint_like( ) : 사이즈를 튜플로 입력하지 않고 기존의 텐서로 정의\n",
    "* torch.randperm( ) : 주어진 범위 내의 정수를 랜덤하게 생성\n",
    "\n",
    "랜덤 생성에 사용되는 시드(seed)는 **torch.manual_seed( )** 명령으로 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4963, 0.7682, 0.0885, 0.1320, 0.3074])\n",
      "tensor([ 0.5507,  0.2704,  0.6472,  0.2490, -0.3354])\n",
      "tensor([8, 4, 3, 6, 9])\n",
      "tensor([1, 3, 4, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "a = torch.rand(5)  # 0과 1 사이의 숫자 균등 생성\n",
    "b = torch.randn(5) # 가우시안 정규분포 생성\n",
    "c = torch.randint(10, size=(5,)) # 0-10까지 5개 정수 균등 생성\n",
    "d = torch.randperm(5) # 0-5 내의 정수 랜덤 생성\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초기화는 **torch.empty( )** 함수를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 1.4714e-43, 2.1867e+23, 2.7450e-06],\n",
       "        [5.3597e-08, 1.6630e+22, 3.3274e+21, 1.6690e-07],\n",
       "        [5.4411e-05, 2.0315e+20, 4.2315e+21, 8.1347e+20]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 특정한 값의 텐서 생성하기\n",
    "\n",
    "다음은 특정한 값으로 채워진 텐서를 생성하는 명령들이다.\n",
    "\n",
    "* torch.arange( ) : 주어진 범위 내의 정수를 순서대로 생성\n",
    "* torch.ones( ) : 주어진 사이즈의 1로 이루어진 텐서 생성\n",
    "* torch.zeros( ) : 주어진 사이즈의 0으로 이루어진 텐서 생성\n",
    "* torch.ones_like( ) : 사이즈를 튜플로 입력하지 않고 기존의 텐서로 정의\n",
    "* torch.zeros_like( ) : 사이즈를 튜플로 입력하지 않고 기존의 텐서로 정의\n",
    "* torch.linspace( ) : 시작점과 끝점을 주어진 갯수만큼 균등하게 나눈 간격점을 행벡터로 출력\n",
    "* torch.logspace( ) : 시작점과 끝점을 주어진 갯수만큼 로그간격으로 나눈 간격점을 행벡터로 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.7143,  1.4286,  2.1429,  2.8571,  3.5714,  4.2857,  5.0000,\n",
       "         5.7143,  6.4286,  7.1429,  7.8571,  8.5714,  9.2857, 10.0000])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(0, 10, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 자료형 변환\n",
    "\n",
    "텐서 내부의 데이터를 변환 할 때는 **.type( )** 메서드를 사용한다. 다음 표는 PyTorch가 지원하는 자료형을 정리한 것이다. GPU tensor는 GPU 연산이 가능한 자료형을 뜻한다. GPU에 관련된 내용에서 자세히 설명한다."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Data type\t             dtype\t                             CPU tensor\t               GPU tensor\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "32-bit floating point\t torch.float32 또는 torch.float\t    torch.FloatTensor\t      torch.cuda.FloatTensor\n",
    "64-bit floating point\t torch.float64 또는 torch.double\t    torch.DoubleTensor\t      torch.cuda.DoubleTensor\n",
    "16-bit floating point\t torch.float16 또는 torch.half\t    torch.HalfTensor\t      torch.cuda.HalfTensor\n",
    "8-bit integer (unsigned) torch.uint8\t                     torch.ByteTensor\t       torch.cuda.ByteTensor\n",
    "8-bit integer (signed)\t torch.int8\t                         torch.CharTensor\t       torch.cuda.CharTensor\n",
    "16-bit integer (signed)\t torch.int16 또는 torch.short\t        torch.ShortTensor\t      torch.cuda.ShortTensor\n",
    "32-bit integer (signed)\t torch.int32 또는 torch.int\t        torch.IntTensor\t          torch.cuda.IntTensor\n",
    "64-bit integer (signed)\t torch.int64 또는 torch.long\t        torch.LongTensor\t      torch.cuda.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]], dtype=torch.int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_tensor.type(dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 형상 변환\n",
    "\n",
    "텐서의 형상을 변환(reshape)하려면 **.view( )** 함수를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.ones(4, 3)\n",
    "t2 = t1.view(3, 4)\n",
    "t3 = t1.view(12)\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서의 차원을 늘리거나 줄일 때도 **.view( )** 함수를 쓴다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.view(1, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**squeeze( )** 명령이나 **unsqueeze( )** 를 사용해도 차원을 변환할 수 있다. **squeeze( ) 함수는 차원의 원소가 1인 차원을 없애주고, unsqueeze( )함수는 인수로 받은 위치에 새로운 차원을 삽입한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4 = torch.rand(1, 3, 3)\n",
    "t4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9527, 0.0362, 0.1852],\n",
       "        [0.3734, 0.3051, 0.9320],\n",
       "        [0.1759, 0.2698, 0.1507]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4.squeeze()  # [3, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5 = torch.rand(3, 3)\n",
    "t5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5.unsqueeze(0).shape # axis=0 위치에 1차원 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0317, 0.2081, 0.9298]],\n",
       "\n",
       "        [[0.7231, 0.7423, 0.5263]],\n",
       "\n",
       "        [[0.2437, 0.5846, 0.0332]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 복수의 텐서 결합\n",
    "\n",
    "복수의 텐서를 결합할 때는 **torch.cat( )**을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(2, 3)\n",
    "b = torch.zeros(3, 3)\n",
    "\n",
    "torch.cat([a, b], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 분할\n",
    "\n",
    "텐서를 여러 개로 나눌 때는 **torch.chunk( )**, **torch.split( )** 명령을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.rand(3, 6)\n",
    "c1, c2, c3 = torch.chunk(c, 3, dim=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5932, 0.1123, 0.1535, 0.2417, 0.7262, 0.7011],\n",
      "        [0.2038, 0.6511, 0.7745, 0.4369, 0.5191, 0.6159],\n",
      "        [0.8102, 0.9801, 0.1147, 0.3168, 0.6965, 0.9143]])\n",
      "tensor([[0.5932, 0.1123],\n",
      "        [0.2038, 0.6511],\n",
      "        [0.8102, 0.9801]])\n",
      "tensor([[0.1535, 0.2417],\n",
      "        [0.7745, 0.4369],\n",
      "        [0.1147, 0.3168]])\n",
      "tensor([[0.7262, 0.7011],\n",
      "        [0.5191, 0.6159],\n",
      "        [0.6965, 0.9143]])\n"
     ]
    }
   ],
   "source": [
    "print(c)\n",
    "print(c1) # shape=(3, 2)\n",
    "print(c2) # shape=(3, 2)\n",
    "print(c3) # shape=(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5932, 0.1123, 0.1535, 0.2417, 0.7262, 0.7011],\n",
      "        [0.2038, 0.6511, 0.7745, 0.4369, 0.5191, 0.6159],\n",
      "        [0.8102, 0.9801, 0.1147, 0.3168, 0.6965, 0.9143]])\n",
      "tensor([[0.5932, 0.1123, 0.1535, 0.2417, 0.7262, 0.7011]])\n",
      "tensor([[0.2038, 0.6511, 0.7745, 0.4369, 0.5191, 0.6159]])\n",
      "tensor([[0.8102, 0.9801, 0.1147, 0.3168, 0.6965, 0.9143]])\n"
     ]
    }
   ],
   "source": [
    "c1, c2, c3 = torch.chunk(c, 3, dim=0) \n",
    "print(c)\n",
    "print(c1) # shape=(1, 6)\n",
    "print(c2) # shape=(1, 6)\n",
    "print(c3) # shape=(1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5932, 0.1123, 0.1535],\n",
      "        [0.2038, 0.6511, 0.7745],\n",
      "        [0.8102, 0.9801, 0.1147]])\n",
      "tensor([[0.2417, 0.7262, 0.7011],\n",
      "        [0.4369, 0.5191, 0.6159],\n",
      "        [0.3168, 0.6965, 0.9143]])\n"
     ]
    }
   ],
   "source": [
    "c1, c2 = torch.split(c, 3, dim=1)\n",
    "print(c1)  # shape=(3, 3)\n",
    "print(c2)  # shape(3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 연산\n",
    "\n",
    "텐서 연산은 파이썬에서 사용하는 연산 기호를 사용하거나 torch의 함수를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 3, 5, 7, 9])\n",
      "tensor([1, 3, 5, 7, 9])\n",
      "tensor([1, 3, 5, 7, 9])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0, 5)\n",
    "z = torch.arange(1, 6)\n",
    "\n",
    "print(x + z)\n",
    "print(torch.add(x, z))\n",
    "print(x.add(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인플레이스 연산\n",
    "\n",
    "명령어 뒤에 **_** 를 붙이면 자기 자신의 값을 바꾸는 인플레이스(inplace) 명령이 된다. 인플레이스 명령은 연산 결과를 반환하면서 동시에 자기 자신의 데이터를 고친다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([1, 3, 5, 7, 9])\n",
      "tensor([1, 3, 5, 7, 9])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0, 5)\n",
    "z = torch.arange(1, 6)\n",
    "print(x)\n",
    "print(x.add_(z))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1개의 원소를 가진 Tensor를 Python의 Scalar로 만들 때는 **.item( )** 함수를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "scl = torch.tensor(1)\n",
    "print(scl.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU 사용\n",
    "\n",
    "이전 데이터 타입에서 보았듯이 GPU 연산을 하기 위해선 텐서를 GPU 연산이 가능한 자료형으로 변환하면 된다. 이에 앞서 본인의 환경에 GPU가 있어야 한다. 사용하는 환경이 GPU를 사용할 수 있는지 확인하려면 **torch.cuda.is_available( )**을 실행 시켜 보면 된다. 만약 GPU를 사용할 준비가 되었다면, True를 반환할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU 연산이 가능한 Tensor를 만드는 것은 device 인수에 GPU 디바이스 객체를 입력하거나 문자열을 입력하면 된다. 디바이스 객체는 **torch.device(\"cuda:0\")**로 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "device"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ts = torch.rand(3, 3, device=device)\n",
    "ts"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tensor([[0.0840, 0.2796, 0.5388],\n",
    "        [0.5165, 0.8742, 0.3991],\n",
    "        [0.4241, 0.4993, 0.0417]], device='cuda:0')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ts = torch.rand(3, 3, device=\"cuda:0\")\n",
    "ts\n",
    "----------------------------------------\n",
    "tensor([[0.4189, 0.6343, 0.2984],\n",
    "        [0.4794, 0.7766, 0.9346],\n",
    "        [0.0996, 0.8428, 0.0675]], device='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존에 있는 Tensor를 GPU 연산이 가능한 자료형으로 바꿀 때는 **.cuda( )** 메서드를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9351, 0.9412, 0.5995],\n",
       "        [0.0652, 0.5460, 0.1872],\n",
       "        [0.0340, 0.9442, 0.8802]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp = torch.rand(3, 3)\n",
    "cp"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cp.cuda()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tensor([[0.2038, 0.6511, 0.7745],\n",
    "        [0.4369, 0.5191, 0.6159],\n",
    "        [0.8102, 0.9801, 0.1147]], device='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autograd\n",
    "\n",
    "autograd는 PyTorch에서 핵심적인 기능을 담당하는 하부 패키지이다. **autograd는 텐서의 연산에 대해 자동으로 미분값을 구해주는 기능을 한다.** 텐서 자료를 생성할 때, **requires_grad** 인수를 True로 설정하거나 **.requires_grad_(True)** 를 실행하면 그 텐서에 행해지는 모든 연산에 대한 미분값을 계산한다. 계산을 멈추고 싶으면 **.detach( )** 함수를 이용하면 된다. \n",
    "\n",
    "예제를 통해 알아 보도록 하자. requires_grad인수를 True로 설정하여 Tensor를 생성했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0012, 0.5936],\n",
      "        [0.4158, 0.4177]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 미분\n",
    "x = torch.rand(2, 2, requires_grad =True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 이 x 에 연산을 수행한다. 다음 코드의 y 는 연산의 결과이므로 미분 함수를 가진다. **grad_fn** 속성을 출력해 미분 함수를 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.2849, grad_fn=<SumBackward0>) <SumBackward0 object at 0x11d3e7ef0>\n"
     ]
    }
   ],
   "source": [
    "y = torch.sum(x * 3)\n",
    "print(y, y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**y.backward( )** 함수를 실행하면 x 의 미분값이 자동으로 갱신된다. x의 **grad** 속성을 확인하면 미분값이 들어 있는 것을 확인 할 수 있다. y 를 구하기 위한 x 의 연산을 수식으로 쓰면 다음과 같다.\n",
    "\n",
    "$$y = \\displaystyle\\sum_{i=1}^4 3 \\times x_i$$\n",
    "\n",
    "이를  𝑥_𝑖 에 대해 미분 하면 미분 함수는 다음과 같다.\n",
    "\n",
    "$$\\dfrac{\\partial y}{\\partial x_i} = 3$$\n",
    "\n",
    "실제 미분값과 같은지 확인해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3.],\n",
       "        [3., 3.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**backward()함수는 자동으로 미분값을 계산해 requires_grad 인수가 True로 설정된 변수의 grad 속성의 값을 갱신한다.** retain_graph 미분을 연산하기 위해서 사용했던 임시 그래프를 유지할 것인가를 설정하는 것이다. \n",
    "\n",
    "기본값은 False로 설정되어 있지만 동일한 연산에 대해 여러 번 미분을 계산하기 위해서는 True로 설정되어 있어야 한다. 그리고 **미분값을 그대로 출력받아 사용하고 싶은 경우에는 torch.autograd.grad( )함수에 출력값과 입력값을 입력하면 미분값을 출력한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3., 3.],\n",
       "         [3., 3.]]),)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상황에 따라 특정 연산에는 미분값을 계산하고 싶지 않은 경우에는 **.detach( )** 함수를 사용한다. 예를 들어, 이전 코드의 결과 값 y에 로지스틱 함수 연산을 수행하고 이에 대한 미분 값을 계산하고 싶지 않은 경우에 다음처럼 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9864)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_1 = y.detach()\n",
    "torch.sigmoid(y_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch를 이용한 선형 회귀 구현\n",
    "\n",
    "지금까지 배운 내용으로 선형회귀를 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>const</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>501</td>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>502</td>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>503</td>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>505</td>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM   ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "501  0.06263  0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527  0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076  0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959  0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741  0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  const  \n",
       "501     21.0  391.99   9.67    1.0  \n",
       "502     21.0  396.90   9.08    1.0  \n",
       "503     21.0  396.90   5.64    1.0  \n",
       "504     21.0  393.45   6.48    1.0  \n",
       "505     21.0  396.90   7.88    1.0  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "df['const'] = np.ones(df.shape[0])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([506, 14]), torch.Size([14, 506]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, m = df.shape\n",
    "X = torch.tensor(df.values)\n",
    "y = torch.tensor(boston.target).view(-1, 1)  # 1차원 추가\n",
    "XT = torch.transpose(X, 0, 1)\n",
    "X.shape, XT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.matmul(torch.matmul(torch.inverse(torch.matmul(XT, X)), XT), y)\n",
    "y_pred = torch.matmul(X, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측한 집값: tensor([18.4061], dtype=torch.float64) 실제 집값: 18.2\n"
     ]
    }
   ],
   "source": [
    "print(\"예측한 집값:\", y_pred[19], \"실제 집값:\", boston.target[19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch를 이용한 퍼셉트론 구현\n",
    "\n",
    "이번에는 붓꽃 분류 문제를 해결하는 퍼셉트론 모형을 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "idx = np.in1d(iris.target, [0, 2]) # label 0, 2 사용\n",
    "x = torch.from_numpy(iris.data[idx, :2]).type(torch.float64)\n",
    "y = torch.from_numpy((iris.target[idx] - 1.0)[:, np.newaxis]).type(torch.float64) # -1, 1로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모형에 사용하는 변수를 **requires_grad** 인수를 True로 하여 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.rand((2, 1), dtype=torch.float64, requires_grad=True)\n",
    "b = torch.rand((1, 1), dtype=torch.float64, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생성한 변수에 직접 연산을 수행해서는 안된다. 예와 같이 \"w\" 라는 변수명으로 변수를 할당하고 이 변수를 연산할 때 파이썬에서 하듯이"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "w = 0.001 * w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 해서는 안된다. 변수에 연산의 결과 값이 들어가면 이후 해당 변수는 이전에 생성한 변수가 아니기 때문에 더 이상 requires_grad 가 True로 설정되지 않는다. **변수에 연산, 할당을 하고 싶을 때는 변수의 data 속성을 불러와 변경한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.data *= 0.001\n",
    "b.data *= 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최적화\n",
    "\n",
    "다음으로 SGD 최적화를 수행한다. **.backward( )**를 사용하면 미분값이 계속 누적된다. 우리는 학습한 뒤 가중치를 수정하고 수정한 가중치에 대한 미분값으로 다시 학습하는 것이기 때문에 가중치를 수정한 뒤에 **.grad_zero_( )** 함수를 호출하여 변수에 할당된 미분값을 0으로 만들어 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 - loss : 48.71629700439473\n",
      " 40 - loss : 49.969721025060096\n",
      " 80 - loss : 49.96085602099961\n",
      "120 - loss : 49.9445647979428\n",
      "160 - loss : 49.90474650797259\n",
      "200 - loss : 49.65989092035962\n",
      "240 - loss : 0.06020430784372478\n",
      "280 - loss : 0.07068318746298043\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 300\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    z = (x.mm(w) + b).tanh()\n",
    "    loss_vec = torch.mul(-y, z)\n",
    "    loss = torch.sum(torch.max(torch.cat([torch.zeros_like(loss_vec), loss_vec], dim=-1), dim=-1)[0], dim=-1)\n",
    "    loss.backward()\n",
    "    w.data -= w.grad * learning_rate\n",
    "    b.data -= b.grad * learning_rate\n",
    "    \n",
    "    if epoch % 40 == 0:\n",
    "        print(\"{:3} - loss : {}\".format(epoch, loss.item()))\n",
    "        \n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  1],\n",
       "       [ 0, 50]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y, [-1 if p < 0 else 1 for p in z.detach().numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch를 이용한 신경망 구현\n",
    "\n",
    "PyTorch 다양한 신경망 모형을 쉽게 구현 할 수 있도록 **torch.nn.module** 을 제공한다. 이를 이용해 간단한 신경망 모형을 구현한다. PyTorch에서는 샘플 데이터를 제공하지 않으므로 샘플데이터는 Keras를 이용해 MNIST 숫자 데이터를 불러오겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist, mnist\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train0, y_train), (X_test0, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) uint8\n",
      "torch.Size([60000]) torch.int64\n",
      "(10000, 28, 28) uint8\n",
      "torch.Size([10000]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train0.shape, X_train0.dtype)\n",
    "print(y_train.shape, y_train.dtype)\n",
    "print(X_test0.shape, X_test0.dtype)\n",
    "print(y_test.shape, y_test.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전에 Keras에서 했던 방식과 같이 전처리를 진행한다. 다만 여기서는 정답 데이터에 대한 원핫 인코딩작업을 하지 않는다. 이유는 바로 다음에 설명할 것이다.\n",
    "\n",
    "전처리를 수행했으면 PyTorch가 사용할 수 있는 텐서로 변환하고 각각 알맞는 데이터 타입으로 변환한다. 이미지 데이터에 대해서는 float 타입, 출력데이터에 대해서는 int64 타입으로 변환한다. 이는 추후 사용할 PyTorch의 함수가 해당 데이터 타입을 지원하기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train0.reshape(-1, 28*28).astype('float64') / 255.0\n",
    "X_test = X_test0.reshape(-1, 28*28).astype('float64') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = map(torch.tensor, (X_train, y_train, X_test, y_test))\n",
    "\n",
    "X_train, y_train, X_test, y_test = X_train.float(), y_train.long(), X_test.float(), y_test.long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.nn.Sequential** 을 이용하면 손쉽게 모델을 구성할 수 있다. 이전 Keras의 방식과 유사하다. 다만 명령어가 조금씩 다를 뿐이다. \n",
    "\n",
    "Keras로 구현한 것과 동일한 구조로 구성한다. **torch.nn.Linear**는 레이어 간의 선형결합을 의미하고 이를 활성화함수에 통과시키면 이전에 배웠던 **MLP(multi layer perceptron)** 구조가 된다. 28 x 28 크기의 이미지를 1차원 벡터로 만들어 입력하고 15개 노드의 은닉층을 거쳐 10개 노드를 출력한다. 이 모형을 GPU를 사용해 학습시키고 싶다면 모형과 사용할 데이터를 GPU연산 가능 자료형으로 변환하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create model\n",
    "from torch import nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 15),  # hidden layer 15개\n",
    "    nn.Sigmoid(),       # 활성화 함수\n",
    "    nn.Linear(15, 10),  # 출력 shape\n",
    "    nn.Sigmoid(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch에서 최적화 알고리즘은 **torch.optim**에 정의되어 있다. 다음처럼 torch.optim 클래스에서 원하는 최적화 방법을 정의한다. \n",
    "\n",
    "여기서는 SGD방법을 사용한다. 그리고 대부분의 비용함수는 torch.nn 클래스에 구현되어 있다. 여기서는 크로스 엔트로피 비용함수(Cross Entropy Loss)를 사용한다. 크로스 엔트로피 비용함수는 신경망 모형을 이용해 분류문제를 풀 때 많이 사용하는 비용함수이다. 이는 추후 신경망 성능개선 다룰 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cost function and optimizer\n",
    "from torch import optim\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=0.5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 epoch, cross entropy : 1.6699703931808472, accuracy : 0.8799999952316284\n",
      "2 epoch, cross entropy : 1.6290602684020996, accuracy : 0.8700000047683716\n",
      "3 epoch, cross entropy : 1.606960415840149, accuracy : 0.8899999856948853\n",
      "4 epoch, cross entropy : 1.5926965475082397, accuracy : 0.8999999761581421\n",
      "5 epoch, cross entropy : 1.5824605226516724, accuracy : 0.9100000262260437\n",
      "6 epoch, cross entropy : 1.5746339559555054, accuracy : 0.9100000262260437\n",
      "7 epoch, cross entropy : 1.5683506727218628, accuracy : 0.9100000262260437\n",
      "8 epoch, cross entropy : 1.5630398988723755, accuracy : 0.9100000262260437\n",
      "9 epoch, cross entropy : 1.5581971406936646, accuracy : 0.9100000262260437\n",
      "10 epoch, cross entropy : 1.5531774759292603, accuracy : 0.9100000262260437\n"
     ]
    }
   ],
   "source": [
    "def accuracy(pred, y):\n",
    "    preds = torch.argmax(pred, dim=1)\n",
    "    return (preds == y).float().mean()\n",
    "\n",
    "# training the model\n",
    "epochs = 10\n",
    "n = X_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(int(n / batch_size)):\n",
    "        start_i = i * batch_size\n",
    "        end_i = start_i + batch_size\n",
    "        xb = X_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    acc = accuracy(pred, yb)\n",
    "    print(\"{} epoch, cross entropy : {}, accuracy : {}\".format(epoch + 1, loss.detach().item(), acc.detach().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hiddenlayer** 패키지를 사용하면 PyTorch로 구현한 모델의 그래프를 시각화 할 수 있다. 설치하는 방법은 다음과 같다.\n",
    "\n",
    "* pip install hiddenlayer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import hiddenlayer as hl\n",
    "\n",
    "hl.build_graph(model=model, args=torch.zeros([10000, 784]).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
