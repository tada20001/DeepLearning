{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망(neural network) 모형은 \n",
    "\n",
    "#### (1) 기저함수(basis function)의 형태를 모수(parameter) 값으로 변화시킬 수 있는 적응형 기저함수 모형(adaptive basis function model)이며, \n",
    "\n",
    "#### (2) 구조적으로는 복수의 퍼셉트론을 쌓아놓은 형태이므로 MLP(multi-layer perceptron)로도 불린다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 퍼셉트론의 복습\n",
    "\n",
    "다음 그림과 같이 독립 변수 벡터가 3차원인 간단한 퍼셉트론 모형을 살펴보자.\n",
    "\n",
    "[그림] 퍼셉트론 모형 : https://datascienceschool.net/upfiles/49d5930259804d938424529ba564ff4c.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 입력  𝑥 \n",
    "\n",
    "$$x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}$$\n",
    "\n",
    "+ 가중치  𝑤\n",
    "$$w = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ w_3 \\end{bmatrix}$$\n",
    "\n",
    "+ 바이어스(y 절편)  𝑏\n",
    "\n",
    "$$b$$\n",
    "\n",
    "+ 활성화 함수 입력값  𝑎\n",
    "\n",
    "$$a = \\sum_{i=1}^3 w_i x_i + b = w^T x + b$$\n",
    "\n",
    "+ 활성화 함수(activation function)  ℎ  와 활성화 함수 출력값  𝑧\n",
    "\n",
    "$$z = h(a) = h \\left( w^Tx + b \\right)$$\n",
    "\n",
    "+ 최종 출력  𝑦̂ \n",
    "\n",
    "$$𝑦̂ = 𝑧$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 시그모이드 활성화 함수\n",
    "\n",
    "일반적으로 활성화 함수  ℎ 로는 위와 아래가 막혀있는(bounded) 시그모이드 함수  𝜎 를 사용하는데 가장 많이 사용하는 활성화 함수는 다음과 같은 로지스틱 함수이다.\n",
    "\n",
    "\\begin{eqnarray} \n",
    "h(a) = \\sigma(a) = \\frac{1}{1+e^{-a}} \n",
    "\\end{eqnarray}\n",
    "\n",
    "시그모이드 함수의 미분은 다음과 같이 쉽게 계산할 수 있다.\n",
    "\n",
    "$$\\dfrac{d\\sigma(a)}{da} = \\sigma(a)(1-\\sigma(a)) = \\sigma(a)\\sigma(-a)$$\n",
    "\n",
    "활성화 함수로 로지스틱 함수를 사용할 때는  𝑧 = ℎ(𝑎)  값으로부터 최종 클래스 결정값  𝑦̂  를 다음 식으로 구한다.\n",
    "\n",
    "$$\\hat{y} = \\text{sign}\\left(z - \\dfrac{1}{2}\\right) = \\text{round}(z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 비선형 기저 함수\n",
    "\n",
    "이런 퍼셉트론에서  𝑥  대신 기저 함수  𝜙(𝑥) 를 사용하면 XOR 문제 등의 비선형 문제를 해결할 수 있다. \n",
    "\n",
    "#### 그러나 고정된 기저 함수를 사용해야 하므로 문제에 맞는 기저 함수를 찾아야 한다는 단점이 있다. 따라서  𝐽 개의 많은 기저 함수를 사용하는 것이 보통이다.\n",
    "\n",
    "$$z = h \\left( \\sum_{j=1}^J w_j \\phi_j(x) + b \\right) = h \\left( w^T \\phi(x) + b \\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 하이퍼 파라미터에 의해 모양이 바뀌는 비선형 기저 함수\n",
    "\n",
    "만약 기저 함수  𝜙(𝑥) 의 형태를 추가적인 모수  𝜃 를 사용하여 조절할 수 있다면 즉, 기저함수  𝜙(𝑥; 𝜃)  를 사용하면  𝜃  값을 바꾸는 것만으로 다양한 모양의 기저 함수를 시도할 수 있다.\n",
    "\n",
    "$$z = h \\left( w^T \\phi(x ; \\theta) + b \\right)$$\n",
    "\n",
    "신경망 즉, MLP(Multi-Layer Perceptron)은 퍼셉트론이 사용하고 있는 로지스틱 시그모이드 함수를 기저 함수로 사용하는 모형이다. 기저 함수의 형태는 하이퍼 파라미터  𝑤^(1) ,  𝑏^(1) 의 값에 따라서 바꿀 수 있다.\n",
    "\n",
    "$$\\phi_j(x ; \\theta_j) = \\phi_j(x ; w^{(1)}_j, {b}^{(1)}_j) \n",
    "= h \\left(w_{j}^{(1)} x + b_j^{(1)} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종 출력값은 다음과 같다.\n",
    "\n",
    "$$z = h \\left( \\sum_{j=1}^M w_j h \\left(w_{j}^{(1)} x + b_j^{(1)} \\right)  + b \\right)$$\n",
    "\n",
    "이 식에서 각각의 계수의 역할은 다음과 같다.\n",
    "\n",
    "+ 𝑤(1), 𝑏(1) : 기저 함수의 모양 조절\n",
    "+ 𝑤, 𝑏 : 결정 함수, 즉 경계면 직선의 모양 조절"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Approximation Theorem\n",
    "\n",
    "Universal Approximation Theorem에 따르면, 위와 같은 적응형 기저 함수  ℎ( 𝑤(1)𝑗 𝑥 + 𝑏(1)𝑗 ) 를 충분히 많이 사용하면 어떠한 형태의 함수와도 유사한 형태의 함수  𝑧(𝑥) 를 만들 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 복수의 퍼셉트론을 사용한 XOR 문제 해결\n",
    "\n",
    "퍼셉트론을 연속적으로 연결하여 비선형 문제를 해결하는 방법은 이미 디지털 회로 설계에서 사용되던 방법이다. 디지털 회로는 AND, OR 등의 디지털 게이트(gate)를 이어서 복잡한 디지털 연산을 하는 회로를 만드는 방법이다."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  INPUT\t             OUTPUT\t\n",
    "  \n",
    "x1\t  x2\tAND\t     NAND\t  XOR\t\n",
    "\n",
    "0\t  0\t      0\t        1\t    0\t\n",
    "0\t  1\t      0\t        1\t    1\t\n",
    "1\t  0\t      0\t        1   \t1\t\n",
    "1\t  1\t      1\t        0   \t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "퍼셉트론의 가중치를 적절히 조정하면 임의의 디지털 게이트를 제작할 수 있다. 예를 들어  𝑤_1 = −2 ,  𝑤_2 = −2 ,  𝑏 = 3  인 퍼셉트론은 NAND 게이트를 구현한다.\n",
    "\n",
    "불리언 로직으로 표현되는 모든 디지털 회로는 NAND 게이트만의 조합으로 구성할 수 있다. 그래서 NAND 게이트를 유니버셜 게이트(universal gate)라고도 부른다.\n",
    "\n",
    "[그림] 퍼셉트론 모형 : https://datascienceschool.net/upfiles/dfc71338d0224f308a5b0188fd148133.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디지털 회로에서는 복수개의 NAND 게이트를 조합하면 어떤 디지털 로직이라도 구현 가능하다. 예를 들어 다음 회로는 두 입력 신호의 합과 자릿수를 반환하는 반가산기(half adder) 회로이다.\n",
    "\n",
    "[그림] 반가산기 회로 : https://datascienceschool.net/upfiles/9ce336ec97444cb9bebd10fdd41ca6c3.png\n",
    "\n",
    "이 퍼셉트론 조합을 보면 4개의 퍼셉트론을 연결하여 XOR 로직을 구현하였음을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다계층 퍼셉트론\n",
    "\n",
    "신경망은 퍼셉트론을 여러 개 연결한 것으로 다계층 퍼셉트론(MLP: Multi-Layer Perceptrons)이라고도 한다. 신경망에 속한 퍼셉트론은 뉴론(neuron) 또는 노드(node)라고 불린다.\n",
    "\n",
    "각 계층(layer)은 다음 계층에 대해 적응형 기저 함수의 역할을 한다. 최초의 계층은 입력 계층(input layer), 마지막 계측은 출력 계층(output layer)이라고 하며 중간은 은닉 계층(hidden layer)라고 한다.\n",
    "\n",
    "[그림] 다계층 퍼셉트론 : https://datascienceschool.net/upfiles/d50a98fcea9748b1b5002c297e9f86b4.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP의 또 다른 특징은 출력 계층에 복수개의 출력 뉴런를 가지고 각 뉴런값으로 출력 클래스의 조건부 확률을 반환하도록 설계하여 멀티 클래스 문제를 해결할 수도 있다는 점이다.\n",
    "\n",
    "다음은 필기 숫자에 대한 영상 정보를 입력 받아 숫자 0 ~ 9 까지의 조건부 확률을 출력하는 MLP의 예이다. 입력 영상이 28 x 28 해상도를 가진다면 입력 계층의 뉴런 수는  28×28=784  개가 된다. 출력은 숫자 0 ~ 9 까지의 조건부 확률을 출력하는  10  개의 뉴런을 가진다.\n",
    "\n",
    "그림의 모형은  15 개의 뉴런을 가지는  1  개의 은닉 계층을 가진다.\n",
    "\n",
    "[그림] 다계층 퍼셉트론 예 : https://datascienceschool.net/upfiles/8d83dafed1ff4805b14e1baf21d68d54.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망 가중치 표기법\n",
    "\n",
    "신경망의 가중치는  𝑤(𝑙) 𝑗,𝑖  과 같이 표기한다. 이 가중치는  𝑙 − 1  번째 계층의  𝑖 번째 뉴런와  𝑙  번째 계층의  𝑗 번째 뉴런을 연결하는 가중치를 뜻한다. 첨자의 순서에 주의한다.\n",
    "\n",
    "[그림] https://datascienceschool.net/upfiles/31be540d70e94029888a7eea11fa554b.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 방식을 사용하면  𝑙−1 번째 레이어의 출력값 벡터  𝑧(𝑙−1) 과  𝑙 번째 레이어의 입력값 벡터  𝑎(𝑙) 간에는 다음과 같은 식이 성립한다.\n",
    "\n",
    "$$a^{(l)} = {W^{(l)}} z^{(l-1)} + b^{(l)}$$\n",
    "\n",
    "#### 이 식에서  𝑊(𝑙) 는  𝑙−1 번째 레이어와  𝑙 번째 레이어의 사이를 연결하는 가중치 값 행렬이고  𝑏(𝑙) 은  𝑙 번째 레이어의 뉴런의 바이어스 값 벡터이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 순방향 전파\n",
    "\n",
    "신경망의 계산 과정은 실제 신경망에서 신호가 전달과는 과정과 유사하므로 순방향 전파(feedforward propagation)라고 한다.\n",
    "\n",
    "𝑙−1 번째 계층의 출력과  𝑙 번째 계층의 출력은 다음과 같은 관계를 가진다.\n",
    "\n",
    "$$a^{(l)} = {W^{(l)}} z^{(l-1)} + b^{(l)}$$\n",
    "\n",
    "$$z^{(l)} = h(a^{(l)})$$\n",
    "\n",
    "하나의 식으로 붙이면 다음과 같다.\n",
    "\n",
    "$$z^{(l)} = h\\left({W^{(l)}} z^{(l-1)} + b^{(l)}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가장 첫번째 레이어 즉, 0번째 레이어의 출력은 입력 데이터 그 자체이다.\n",
    "\n",
    "$$z^{(0)} = x$$\n",
    "\n",
    "#### 가장 마지막 레이어 즉  𝐿 번째 레이어의 출력은 최종 출력이 된다.\n",
    "\n",
    "$$\\hat{y} = z^{(L)}$$\n",
    "\n",
    "아래에 순방향 전파의 예를 보였다.\n",
    "\n",
    "### 단계 1\n",
    "[그림] https://datascienceschool.net/upfiles/5a961a01f67e494e9136fcd7af4a4a2a.png\n",
    "\n",
    "### 단계 2\n",
    "[그림] https://datascienceschool.net/upfiles/0aa0e3f0664f4153a7d24009362a7b00.png\n",
    "\n",
    "$$z^{(1)} = h \\left( {W^{(1)}} x + b^{(1)} \\right) = h \\left( a^{(1)} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단계 3\n",
    "[그림] https://datascienceschool.net/upfiles/d6fc15e89370416db5f53f6d1951aae3.png\n",
    "\n",
    "$$z^{(2)} = h \\left( {W^{(2)}} z^{(1)} + b^{(2)} \\right) = h \\left( a^{(2)} \\right)$$\n",
    "\n",
    "### 단계 4\n",
    "[그림] https://datascienceschool.net/upfiles/f038981fd189426b9692d3fac833d6d8.png\n",
    "\n",
    "$$\\hat{y} = z^{(3)} = h \\left( {W^{(3)}} z^{(2)} + b^{(3)} \\right) = h \\left( a^{(3)} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오차함수\n",
    "\n",
    "신경망의 오차함수는 조건부 확률이라는 실수 값을 출력해야 하므로 퍼셉트론과 달리 제곱합 오차함수를 사용한다.\n",
    "\n",
    "\\begin{eqnarray}  C(w,b) = \\sum_{i=1}^N C_i(w,b) =  \\sum_{i=1}^N \\| y_i - z_i^{(L)}(w,b) \\|^2 \n",
    "\\end{eqnarray}\n",
    "\n",
    "이 때  𝑁 은 학습용 데이터의 갯수이다.\n",
    "\n",
    "#### 로지스틱 활성 함수를 이용한 분류 문제를 풀 때는 정답  𝑦 가 클래스  𝑘 에 속하는 데이터에 대해  𝑘 번째 값만 1이고 나머지는 0인 원핫인코딩(one-hot-encoding) 벡터이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가중치 최적화\n",
    "\n",
    "오차함수를 최소화하는 최적의 가중치를 찾기 위해 다음과 같이 미분(gradient)을 사용한 최급강하법(Steepest Gradient Descent)을 적용한다. 가중치 갱신 공식은 다음과 같다. 여기에서  𝜇 는 최적화 스텝사이즈(step size)이다.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "  w_{k+1}  &=& w_{k} - \\mu \\frac{\\partial C}{\\partial w} \\\\\n",
    "  b_{k+1} &=& b_{k} - \\mu \\frac{\\partial C}{\\partial b}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 역전파\n",
    "\n",
    "단순하게 수치적으로 미분을 계산한다면 모든 가중치에 대해서 개별적으로 미분을 계산해야 한다. \n",
    "#### 그러나 역전파(back propagation) 방법을 사용하면 모든 가중치에 대한 미분값을 한번에 계산할 수 있다.\n",
    "\n",
    "역전파 방법을 수식으로 표현하면 다음과 같다.\n",
    "\n",
    "#### (1) 우선  𝛿 는 다음과 같이 정의한다.\n",
    "\n",
    "오차함수  𝐶 를  𝑎 로 미분한 것을  𝛿 라고 한다.\n",
    "\n",
    "$$\\delta_j^{(l)} = \\dfrac{\\partial C}{\\partial a_j^{(l)}}$$\n",
    "\n",
    "#### (2) 가장 최종단의  𝛿 를 구한다.\n",
    "\n",
    "최종단의  𝛿 는 다음과 같이 예측 오차 자체다. 따라서 역전파를 오차 역전파(error back propagation)라고도 한다.\n",
    "\n",
    "$$\\delta^{(L)}_j = y_j - z_j$$\n",
    "\n",
    "#### (3) 다음 식을 사용하여  𝛿 를 뒤에서 앞으로 전파한다.\n",
    "\n",
    "$$\\delta^{(l-1)}_j = h'(a^{(l-1)}_j) \\sum_{i=1}^{N_{(l)}} w^{(l)}_{ij} \\delta^{(l)}_i$$\n",
    "\n",
    "위 식에서  𝑁(𝑙) 는  𝑙 번째 레이어의 노드의 갯수이다.\n",
    "\n",
    "이 식을 벡터-행렬 식으로 쓰면 다음과 같다.\n",
    "\n",
    "$$\\delta^{(l-1)} = h'(a^{(l-1)}) \\odot ({W^T}^{(l)} \\delta^{(l)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기에서  ⊙  연산 기호는 Hadamard Product, Schur product, 혹은 element-wise product 라고 불리는 연산으로 정의는 다음과 같다. 즉 NumPy의 일반적인 배열 곱과 같다.\n",
    "\n",
    "$$x \\odot y = \n",
    "\\left(\\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\end{array}\\right) \\odot\n",
    "\\left(\\begin{array}{c} y_1 \\\\ y_2 \\\\ y_3 \\end{array}\\right) \n",
    "= \\left(\\begin{array}{c} x_1 y_1 \\\\ x_2 y_2 \\\\ x_3 y_3 \\end{array}\\right)$$\n",
    "\n",
    "### 단계 1\n",
    "\n",
    "[그림] https://datascienceschool.net/upfiles/412c6a4d9fef40329162132afd502730.png\n",
    "\n",
    "$$\\delta^{(3)} = y - z^{(3)}$$\n",
    "\n",
    "### 단계 2\n",
    "\n",
    "[그림] https://datascienceschool.net/upfiles/65380b490f654433b73b17aeef2c416d.png\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^{(3)}_{jk}} = z^{(2)}_k \\delta^{(3)}_j$$\n",
    "\n",
    "$$\\delta^{(2)} = h'(a^{(2)}) \\odot ((W^{(3)})^T \\delta^{(3)})$$\n",
    "\n",
    "### 단계 3\n",
    "\n",
    "[그림] https://datascienceschool.net/upfiles/911b7558a3904432b2d07a86e834fe84.png\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^{(2)}_{jk}} = z^{(1)}_k \\delta^{(2)}_j$$\n",
    "\n",
    "$$\\delta^{(1)} = h'(a^{(1)}) \\odot ((W^{(2)})^T \\delta^{(2)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단계 4\n",
    "\n",
    "[그림] https://datascienceschool.net/upfiles/d4a4ad983d2340cbabb6059c3a119290.png\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^{(1)}_{jk}} = x_k \\delta^{(1)}_j$$\n",
    "\n",
    "#### (4) 가중치에 대한 미분은 다음과 같이 구한다.\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^{(l)}_{ji}} = \\delta^{(l)}_j z^{(l-1)}_i$$\n",
    "\n",
    "따라서 오차값을 위 식에 따라 앞쪽으로 다시 전파하면 전체 가중치에 대한 미분을 구할 수 있다.\n",
    "\n",
    "또 바이어스에 대한 미분은  𝛿 와 같다.\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^{(l)}_{j}} = \\delta^{(l)}_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD\n",
    "\n",
    "위에서 구한 오차함수 값의 그레디언트는 데이터 한 쌍 즉  (𝑥_𝑖, 𝑦_𝑖) 만을 이용하여 구한 값이다. 실제 오차함수  𝐶 는 모든 개별 데이터에 대해 구한 오차함수 값  𝐶_𝑖 의 합이다.\n",
    "\n",
    "$$C = \\sum_{i=1}^N C_i$$\n",
    "\n",
    "#### 따라서 전체 오차함수 값에 대한 그레디언트 값은 각각의 개별 데이터에 대해 구한 그레디언트 값의 합이다.\n",
    "\n",
    "$$\\dfrac{\\partial C}{\\partial w_k} = \\sum_{i=1}^N \\dfrac{\\partial C_i}{\\partial w_k}$$\n",
    "\n",
    "이 그레디언트 값이 구해지면 가중치를 업데이트할 수 있다.\n",
    "\n",
    "$$w_{k+1} = w_k - \\mu \\dfrac{\\partial C}{\\partial w_k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데 트레이닝 데이터의 수가 많은 경우 모든 데이터를 다 사용하여 그레디언트를 구하면 그레디언트를 한 번 구하는데, 즉, 가중치를 한 번 업데이트하는데 드는 계산 시간이 너무 길어지므로 자주 업데이트를 할 수 없고 따라서 최종 가중치 값을 구하는데 시간이 너무 오래 걸린다.\n",
    "\n",
    "#### 따라서 일부 데이터만 사용하여 일단 그레디언트의 근사값을 구하고\n",
    "\n",
    "$$\\tilde{\\dfrac{\\partial C}{\\partial w_k}} = \\dfrac{N}{M} \\sum_{i=1}^M \\dfrac{\\partial C_i}{\\partial w_k} \\;\\;\\;\\; (M < N)$$\n",
    "\n",
    "#### 이 근사값을 이용하여 가중치를 업데이트하는 방법 즉 SGD(Stochastic Gradient Descent)방법을 사용한다.\n",
    "\n",
    "$$w_{k+1} = w_k - \\mu \\tilde{\\dfrac{\\partial C}{\\partial w_k}}$$\n",
    "\n",
    "SGD방법에서 그레디언트를 한 번 바꾸기 위해 사용하는 데이터의 갯수  𝑀 을 미니 배치 크기(mini-batch size)라고 한다. \n",
    "\n",
    "그리고 그레디언트를 업데이트한 뒤에는 이미 사용한 데이터가 아닌 다른 데이터에서 미니 배치 크기 만큼의 데이터를 뽑아서 사용한다. \n",
    "\n",
    "#### 이렇게 하면 전체 데이터가  𝑁 개, 미니배치크기가  𝑀 일 때  𝑁𝑀 번 그레디언트를 업데이트하면 모든 데이터를 사용하게 된다. 이것을 1 에포크(epoch)라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망 정리\n",
    "\n",
    "신경망의 계수는 다음과 같이 찾는다.\n",
    "\n",
    "#### 1. 초기화(initialize)\n",
    "+ 모든  𝑤 ,  𝑏  값을 임의의 값으로 초기화\n",
    "\n",
    "#### 2. 입력 데이터(input)\n",
    "+ 하나의 표본데이터  𝑥_𝑖 로 입력 계층 설정\n",
    "\n",
    "#### 3. 순방향 전파(feedforward propagation)\n",
    "+ 모든 뉴런에 대해  𝑎 ,  𝑧 값 계산\n",
    "\n",
    "#### 4. 오차 계산(output and error calculation)\n",
    "+ 최종 출력 계층의 값  𝑧(𝐿)  및 오차  𝛿(𝐿)  계산\n",
    "\n",
    "#### 5. 오차 역전파(backpropagation)\n",
    "+ 반대 방향으로 오차  𝛿  전파\n",
    "\n",
    "#### 6. 그레디언트 계산(gradient calculation)\n",
    "+ 표본데이터  𝑥_𝑖 에 의한 오차함수의 미분값(그레디언트)  ∂𝐶_𝑖 / ∂𝑤 = 𝑧 𝛿 ,  ∂𝐶_𝑖 / ∂𝑏 = 𝛿   계산\n",
    "\n",
    "#### 7. 반복(minibatch-size iteration)\n",
    "+ 표본 데이터를  𝑥_𝑖+1 로 바꾸어 미니배치크기 만큼 2~6 단계를 반복\n",
    "\n",
    "#### 8. 가중치 갱신(weight update)\n",
    "+ 미니배치크기 만큼의 데이터를 사용한 후 그레디언트  ∂𝐶 / ∂𝑤 ,  ∂𝐶 / ∂𝑏  계산\n",
    "+ 이 그레디언트 값으로  𝑤 ,  𝑏  값을 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
