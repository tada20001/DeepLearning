{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'o': 0,\n",
       " ' ': 1,\n",
       " 'f': 2,\n",
       " 'w': 3,\n",
       " 'n': 4,\n",
       " 'i': 5,\n",
       " 'y': 6,\n",
       " 't': 7,\n",
       " 'u': 8,\n",
       " 'a': 9}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \" if you want you\"\n",
    "idx2char = list(set(sample))\n",
    "char2idx = {c: i for i, c in enumerate(idx2char)}\n",
    "char2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 5, 2, 1, 6, 0, 8, 1, 3, 9, 4, 7, 1, 6, 0, 8]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_idx = [char2idx[c] for c in sample]\n",
    "sample_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"one_hot:0\", shape=(?, 15, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x_data = [sample_idx[:-1]]\n",
    "y_data = [sample_idx[1:]]\n",
    "\n",
    "sequence_length = len(x_data[0])  # 문자 수\n",
    "num_classes = len(char2idx) # hidden_size \n",
    "batch_size = 1\n",
    "\n",
    "X = tf.placeholder(tf.int32, shape=[None, sequence_length])\n",
    "Y = tf.placeholder(tf.int32, shape=[None, sequence_length])\n",
    "X_one_hot = tf.one_hot(X, num_classes)\n",
    "print(X_one_hot) # shape의 axis=2 은 one_hot, axis=1는 hidden_size(입출력 글자 수), axis=0은 batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length, num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=num_classes, state_is_tuple=True)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X_one_hot, initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "### loss function and optimizer\n",
    "weights = tf.ones([batch_size, sequence_length]) # 출력 shape\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(weights=weights, logits=outputs, targets=Y)\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.1).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(outputs, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Loss: 2.293302059 prediction: u  uuuuuuu  uuu\n",
      "50 Loss: 1.072835445 prediction: if you want you\n",
      "100 Loss: 1.048169255 prediction: if you want you\n",
      "150 Loss: 1.044602275 prediction: if you want you\n",
      "200 Loss: 1.042868257 prediction: if you want you\n",
      "250 Loss: 1.042195201 prediction: if you want you\n",
      "300 Loss: 1.041774511 prediction: if you want you\n",
      "350 Loss: 1.041486025 prediction: if you want you\n",
      "400 Loss: 1.041276097 prediction: if you want you\n",
      "450 Loss: 1.041103005 prediction: if you want you\n",
      "500 Loss: 1.026245475 prediction: if you want you\n",
      "550 Loss: 1.026037097 prediction: if you want you\n",
      "600 Loss: 1.025948882 prediction: if you want you\n",
      "650 Loss: 1.025881171 prediction: if you want you\n",
      "700 Loss: 1.025825858 prediction: if you want you\n",
      "750 Loss: 1.025779247 prediction: if you want you\n",
      "800 Loss: 1.025739670 prediction: if you want you\n",
      "850 Loss: 1.025705695 prediction: if you want you\n",
      "900 Loss: 1.025676012 prediction: if you want you\n",
      "950 Loss: 1.025649905 prediction: if you want you\n",
      "1000 Loss: 1.025626898 prediction: if you want you\n",
      "1050 Loss: 1.025606513 prediction: if you want you\n",
      "1100 Loss: 1.025588274 prediction: if you want you\n",
      "1150 Loss: 1.025571704 prediction: if you want you\n",
      "1200 Loss: 1.025557041 prediction: if you want you\n",
      "1250 Loss: 1.025543809 prediction: if you want you\n",
      "1300 Loss: 1.025531769 prediction: if you want you\n",
      "1350 Loss: 1.025520921 prediction: if you want you\n",
      "1400 Loss: 1.025511026 prediction: if you want you\n",
      "1450 Loss: 1.025501847 prediction: if you want you\n",
      "1500 Loss: 1.025493622 prediction: if you want you\n",
      "1550 Loss: 1.025485992 prediction: if you want you\n",
      "1600 Loss: 1.025478959 prediction: if you want you\n",
      "1650 Loss: 1.025472403 prediction: if you want you\n",
      "1700 Loss: 1.025466442 prediction: if you want you\n",
      "1750 Loss: 1.025460958 prediction: if you want you\n",
      "1800 Loss: 1.025455832 prediction: if you want you\n",
      "1850 Loss: 1.025451064 prediction: if you want you\n",
      "1900 Loss: 1.025446534 prediction: if you want you\n",
      "1950 Loss: 1.025442243 prediction: if you want you\n",
      "2000 Loss: 1.025438190 prediction: if you want you\n",
      "2050 Loss: 1.025434494 prediction: if you want you\n",
      "2100 Loss: 1.025431156 prediction: if you want you\n",
      "2150 Loss: 1.025427818 prediction: if you want you\n",
      "2200 Loss: 1.025424719 prediction: if you want you\n",
      "2250 Loss: 1.025421858 prediction: if you want you\n",
      "2300 Loss: 1.025418997 prediction: if you want you\n",
      "2350 Loss: 1.025416613 prediction: if you want you\n",
      "2400 Loss: 1.025414109 prediction: if you want you\n",
      "2450 Loss: 1.025411606 prediction: if you want you\n",
      "2500 Loss: 1.025409460 prediction: if you want you\n",
      "2550 Loss: 1.025413990 prediction: if you want you\n",
      "2600 Loss: 1.025410533 prediction: if you want you\n",
      "2650 Loss: 1.025414586 prediction: if you want you\n",
      "2700 Loss: 1.025401711 prediction: if you want you\n",
      "2750 Loss: 1.025401115 prediction: if you want you\n",
      "2800 Loss: 1.025399327 prediction: if you want you\n",
      "2850 Loss: 1.025406122 prediction: if you want you\n",
      "2900 Loss: 1.025395274 prediction: if you want you\n",
      "2950 Loss: 1.025400043 prediction: if you want you\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(3000):\n",
    "    l, _ = sess.run([loss, train], feed_dict={X: x_data, Y: y_data})\n",
    "    result = sess.run(prediction, feed_dict={X: x_data})\n",
    "    result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(i, \"Loss:\", \"%.9f\" % l, \"prediction:\", \"\".join(result_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real long sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_set = list(set(sentence))\n",
    "char_dic = {c:idx for idx, c in enumerate(char_set)}\n",
    "\n",
    "sentence_idx = [char_dic[c] for c in sentence]\n",
    "seq_length = len(\"if you wan\") # 10개\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, len(sentence) - seq_length):\n",
    "    x_str = sentence[i:i+seq_length]  # 10개씩 170 sequences\n",
    "    y_str = sentence[i+1:i+seq_length+1]\n",
    "    #print(i, x_str, '->', y_str) #\n",
    "    \n",
    "    x = [char_dic[c] for c in x_str]\n",
    "    y = [char_dic[c] for c in y_str]\n",
    "    \n",
    "    dataX.append(x)\n",
    "    dataY.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 25, 170, 10)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(char_dic)\n",
    "hidden_size = len(char_set)\n",
    "batch_size = len(dataX)\n",
    "sequence_length = seq_length\n",
    "num_classes, hidden_size, batch_size, seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"one_hot:0\", shape=(?, 10, 25), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.int32, shape=[None, sequence_length])\n",
    "Y = tf.placeholder(tf.int32, shape=[None, sequence_length])\n",
    "X_one_hot = tf.one_hot(X, num_classes)\n",
    "print(X_one_hot)\n",
    "\n",
    "# LSTM (logits)\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, _state = tf.nn.dynamic_rnn(cell, X_one_hot, initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "# Loss and optimizer\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=outputs, targets=Y, weights=weights)\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.1).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(outputs, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Loss: 3.222452402 prediction: t         \n",
      "100 Loss: 2.067435265 prediction: t         \n",
      "200 Loss: 1.991209507 prediction: t y u aant\n",
      "300 Loss: 1.953233838 prediction: t you aant\n",
      "400 Loss: 1.944414139 prediction: t you aant\n",
      "500 Loss: 1.944022179 prediction: t you aant\n",
      "600 Loss: 1.934392333 prediction: t you aant\n",
      "700 Loss: 1.920397878 prediction: t you aant\n",
      "800 Loss: 1.914296985 prediction: t you aant\n",
      "900 Loss: 1.906684160 prediction: t you aant\n",
      "1000 Loss: 1.918459415 prediction: t you aant\n",
      "1100 Loss: 1.904967904 prediction: t you aant\n",
      "1200 Loss: 2.007092237 prediction: m you aant\n",
      "1300 Loss: 1.964319944 prediction: m you aant\n",
      "1400 Loss: 1.946165204 prediction: m you aant\n",
      "1500 Loss: 1.966137767 prediction: t youtaand\n",
      "1600 Loss: 1.918052077 prediction: t you aant\n",
      "1700 Loss: 1.908257484 prediction: t you aant\n",
      "1800 Loss: 1.981823087 prediction: l youteant\n",
      "1900 Loss: 1.929846644 prediction: m you aant\n",
      "2000 Loss: 1.921315312 prediction: m you aant\n",
      "2100 Loss: 1.932625532 prediction: m you aant\n",
      "2200 Loss: 1.913955569 prediction: m you aant\n",
      "2300 Loss: 1.948893189 prediction: m youtaant\n",
      "2400 Loss: 1.916236877 prediction: m you aant\n",
      "2500 Loss: 1.908206820 prediction: m youtaant\n",
      "2600 Loss: 1.904813409 prediction: m youtaant\n",
      "2700 Loss: 1.904255867 prediction: l youtaant\n",
      "2800 Loss: 1.902298331 prediction: m youtaant\n",
      "2900 Loss: 1.899971724 prediction: m youtaant\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(3000):\n",
    "    l, _ = sess.run([loss, train], feed_dict={X: dataX, Y: dataY})\n",
    "    result = sess.run(prediction, feed_dict={X: dataX})\n",
    "    result_str = [char_set[c] for c in np.squeeze(result[0])]\n",
    "    if i % 100 == 0:\n",
    "        print(i, \"Loss:\", \"%.9f\" % l, \"prediction:\", \"\".join(result_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170, 10)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l toutaant\n",
      " eoutaant \n",
      "tou aant t\n",
      "ou aant to\n",
      "r dont to \n",
      " pant to t\n",
      "tont to tu\n",
      "ant to eut\n",
      "ss torlutl\n",
      "  th lutld\n",
      "hao tutld \n",
      "to lutld a\n",
      "h lutld a \n",
      "rlutld a t\n",
      "tutld a th\n",
      "utld a thi\n",
      " pd a thip\n",
      "ld anthip,\n",
      "d anthip, \n",
      " anthip, d\n",
      "tsthip, do\n",
      "sship, don\n",
      "thip, dont\n",
      "iip, dontt\n",
      "et, dontt \n",
      "ll dontt a\n",
      "  dondt dr\n",
      " dordt dra\n",
      "ton t aram\n",
      " n t aram \n",
      "rgt drum t\n",
      " t dram tp\n",
      "t aram tp \n",
      "haoam tp p\n",
      "toat tp pe\n",
      "    tp peo\n",
      "   tp peop\n",
      "  up peope\n",
      " tp peopee\n",
      "t  peopee \n",
      "  peopee t\n",
      " peopee to\n",
      "tpopee tor\n",
      "  pee tore\n",
      "npee toret\n",
      "r e to  th\n",
      " e torethe\n",
      "d to ethe \n",
      "nto ethe  \n",
      "to ethe  t\n",
      "h  the  to\n",
      "r the  tor\n",
      "ethe  to l\n",
      "nhe  te lo\n",
      "he  te lol\n",
      "er te loll\n",
      "n th lolle\n",
      " th lolle \n",
      "to lolle t\n",
      "h lolle t \n",
      "rlollest t\n",
      "tolle t to\n",
      "tlle t tor\n",
      "reest tord\n",
      "dest tord \n",
      "d t tood a\n",
      "nt oord an\n",
      "thoord and\n",
      "haord and \n",
      "tord and d\n",
      "ard and do\n",
      "rd and don\n",
      "r and dont\n",
      " ant dontt\n",
      "tsd dontt \n",
      "ss dontt a\n",
      "  dondt ds\n",
      " ao  t ass\n",
      "ton t assi\n",
      " n t assim\n",
      "rgt dssimn\n",
      " t dssimn \n",
      "t assimn t\n",
      "hassit  th\n",
      "tssit  the\n",
      "ss t  the \n",
      "iit  the  \n",
      "it  the  t\n",
      "l  the  te\n",
      "e the  tes\n",
      " t e  toss\n",
      "toe  tesss\n",
      "he  tesss \n",
      "er tesss a\n",
      "n tesssian\n",
      " tosss and\n",
      "tosssiand \n",
      "hsss and t\n",
      "sss and to\n",
      "is and tor\n",
      "ssand tor \n",
      "itnd tor  \n",
      "tsd dor   \n",
      "ss dor   t\n",
      "  dor   tu\n",
      " aar   tut\n",
      "tor   tut \n",
      "ar a tut r\n",
      "r a tut ra\n",
      " t tut rat\n",
      "s tut rath\n",
      " dut rathe\n",
      "tut aathe \n",
      "ut aathe  \n",
      "  aathe  t\n",
      "haathe  te\n",
      "tuthe  tea\n",
      " the  teas\n",
      "she  tea t\n",
      "he  tea th\n",
      "er teattht\n",
      "n tha thth\n",
      " tha ththe\n",
      "toast the \n",
      "hrst the  \n",
      "nst the  t\n",
      "st the  te\n",
      "t toe  te \n",
      "ethe  te l\n",
      "toe  te lo\n",
      "he  te lon\n",
      "er te lon \n",
      "n te lont \n",
      " to lont t\n",
      "to lont to\n",
      "h lont tor\n",
      "rlont tor \n",
      "tond tor t\n",
      "d   tor th\n",
      "rg tor the\n",
      "  tor the \n",
      "eto  the t\n",
      "tor the tn\n",
      "   the tnd\n",
      "r the tnd \n",
      " the tnd e\n",
      "toe tnd es\n",
      "he tnd ess\n",
      "ertnd ess \n",
      "ntnd ess t\n",
      "tos ess am\n",
      "ns ess amm\n",
      "  d s amme\n",
      " ons ammen\n",
      "d s ammens\n",
      "ns ammensi\n",
      "iiammensit\n",
      "itlmensit \n",
      "tlmensit  \n",
      "lmensit  o\n",
      "  nsit  o \n",
      " nsit  o  \n",
      "nsit  o  t\n",
      "  th o  th\n",
      "it  o  the\n",
      "l  o  the \n",
      "hotu the t\n",
      "oou the ti\n",
      "t  thestea\n",
      "r the tea \n"
     ]
    }
   ],
   "source": [
    "for i in range(170):\n",
    "    print(\"\".join([char_set[c] for c in result[i, :]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과적으로 예측이 잘 안됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
