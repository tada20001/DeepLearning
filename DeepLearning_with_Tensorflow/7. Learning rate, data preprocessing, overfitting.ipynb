{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate 검토\n",
    "\n",
    "* Large learning rate : 큰 값을 정할 경우, overshooting이 발생한다. 코스트 함수 값이 수렴하지 않고 발산할 경우 learning rate 값이 크게 설정되었다고 볼 수 있다. 이러한 경우에는 learning rate를 작게 설정하여 트레이닝하는 것이 좋다.\n",
    "$$$$\n",
    "* Small learning rate : 수렴하는 데까지 오래 걸리거나 local minimum에 도달할 가능성이 있다. 이 경우에는 반대로 learning rate를 크게 설정한다.\n",
    "\n",
    "#### Try several Learning rates\n",
    "* cost function 값 검토\n",
    "* 적절하게 감소하는지 검토\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터의 전처리 필요\n",
    "\n",
    "* x1, x2 값들이 스케일 차이가 클 경우, 한쪽 방향으로 치우친 weight 값들이 나올 수 있다. 이럴 경우 적절한 weight를 찾기 어렵다.\n",
    "* 이 경우에는 데이터의 정규화 혹은 일반화가 필요하다.\n",
    "* 트레이닝 코스트 함수값이 수렴하지 않을 경우는 데이터의 정규화가 필요한지 확인해야 한다."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_std[:, 0] = (x[:, 0] - x[:, 0].mean()) / x[:, 0].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "* 모델이 학습용 데이터에 너무 잘 맞지만, 테스트용 데이터에는 성능이 떨어지는 경우에는 overfitting을 검토해야 한다.\n",
    "* 이 경우에는 (1) 트레이닝 데이터를 늘리고 (2) feature의 수를 줄인다. 그리고 (3) 일반화(Regularization)를 검토해야 한다.\n",
    "* 여기에서 일반화는 cost function에 아래와 같은 항을 추가한다.\n",
    "\n",
    "$$Loss = \\dfrac{1}{N} \\sum_i D(S(Wx_i+b), L_i) + \\lambda\\sum W^2$$\n",
    "\n",
    "* w 값이 클수록 overfitting에 원인이 되므로, weight의 각 원소들을 곱하여 합한다. 위의 $\\lambda$를 regularization strength라고 한다.\n",
    "\n",
    "* 일반화를 tensorflow로 구현하면 다음의 코드를 cost function에 추가한다."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ㅣ2reg = 0.001 * tf.reduce_sum(tf.square(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델의 성능을 확인하는 방법\n",
    "* 테스트용 데이터로 예측\n",
    "* training set / validation set(일반화, learning-rate 용) / testing set\n",
    "* 데이터가 많을 때, Online learning이라는 방법을 사용한다. 기존의 데이터를 학습하고 기존의 데이터를 같이 트레이닝하는 것이 아니라 새롭게 추가된 데이터만를 학습하는 방법이다.\n",
    "$$$$\n",
    "* Accuracy 측정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예제 1. Training and Test datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2, 1], [1, 3, 2], [1, 3, 4], [1, 5, 5], [1, 7, 5], [1, 2, 5], [1, 6, 6], [1, 7, 7]]\n",
    "y_data = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]]\n",
    "\n",
    "x_test = [[2, 1, 1], [3, 1, 2], [3, 3, 4]]\n",
    "y_test = [[0, 0, 1], [0, 0, 1], [0, 0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/imjunghee/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/imjunghee/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "0 9.034454 [[ 0.07486635  2.0733237   0.02927919]\n",
      " [-0.327771   -0.5641476  -0.13909142]\n",
      " [-0.36659992  0.9159789   1.7506251 ]]\n",
      "1 5.6128826 [[ 0.09969777  2.0957494  -0.01797797]\n",
      " [-0.16563602 -0.4265442  -0.43882978]\n",
      " [-0.2042978   1.0753073   1.4289947 ]]\n",
      "2 3.4595628 [[ 0.12441412  2.0621054  -0.00905032]\n",
      " [-0.00380576 -0.5465284  -0.48067582]\n",
      " [-0.04219662  0.9719735   1.3702272 ]]\n",
      "3 2.6487746 [[ 0.14847937  2.0426042  -0.0136143 ]\n",
      " [ 0.15596874 -0.59276366 -0.5942151 ]\n",
      " [ 0.11845529  0.9411244   1.2404244 ]]\n",
      "4 1.8749418 [[ 1.6955191e-01  2.0099068e+00 -1.9893646e-03]\n",
      " [ 3.0084687e-01 -7.1068048e-01 -6.2117642e-01]\n",
      " [ 2.6727593e-01  8.3604050e-01  1.1966877e+00]]\n",
      "5 1.3754466 [[ 0.17226504  2.0023313   0.00287301]\n",
      " [ 0.34046137 -0.69622505 -0.67524636]\n",
      " [ 0.32607394  0.8550722   1.118858  ]]\n",
      "6 1.3182291 [[ 0.16452749  1.9900743   0.02286755]\n",
      " [ 0.3179871  -0.6994014  -0.64959574]\n",
      " [ 0.32685748  0.85005546  1.1230912 ]]\n",
      "7 1.299587 [[ 0.15926276  1.9792223   0.03898422]\n",
      " [ 0.31010723 -0.69771385 -0.64340335]\n",
      " [ 0.34122634  0.85154027  1.1072376 ]]\n",
      "8 1.285988 [[ 0.15355447  1.9685353   0.05537955]\n",
      " [ 0.299645   -0.69526476 -0.6353903 ]\n",
      " [ 0.35297102  0.8535238   1.0935093 ]]\n",
      "9 1.272634 [[ 0.1480246   1.9578086   0.07163608]\n",
      " [ 0.29018563 -0.6933342  -0.62786144]\n",
      " [ 0.36544362  0.8549458   1.0796148 ]]\n",
      "10 1.2594817 [[ 0.14245366  1.9472057   0.08780994]\n",
      " [ 0.28047964 -0.69106054 -0.6204291 ]\n",
      " [ 0.37747627  0.8566231   1.0659049 ]]\n",
      "11 1.2465293 [[ 0.13693213  1.9366406   0.10389649]\n",
      " [ 0.2710387  -0.68889904 -0.6131497 ]\n",
      " [ 0.3895437   0.85811335  1.0523472 ]]\n",
      "12 1.2337774 [[ 0.13141967  1.9261589   0.11989067]\n",
      " [ 0.26163518 -0.6866275  -0.6060177 ]\n",
      " [ 0.401432    0.85963625  1.038936  ]]\n",
      "13 1.221227 [[ 0.12593535  1.9157412   0.13579272]\n",
      " [ 0.2523765  -0.68435496 -0.59903157]\n",
      " [ 0.4132395   0.8610856   1.0256792 ]]\n",
      "14 1.2088785 [[ 0.12047018  1.9053994   0.15159963]\n",
      " [ 0.2432122  -0.6820305  -0.5921917 ]\n",
      " [ 0.42491755  0.86251295  1.0125738 ]]\n",
      "15 1.1967323 [[ 0.11502866  1.8951304   0.16731018]\n",
      " [ 0.2341676  -0.6796806  -0.585497  ]\n",
      " [ 0.43648818  0.86389357  0.9996226 ]]\n",
      "16 1.1847887 [[ 0.10960868  1.8849381   0.18292238]\n",
      " [ 0.2252311  -0.677294   -0.57894707]\n",
      " [ 0.44793913  0.86523986  0.98682535]]\n",
      "17 1.1730478 [[ 0.10421136  1.8748231   0.19843471]\n",
      " [ 0.21640916 -0.6748781  -0.572541  ]\n",
      " [ 0.4592751   0.8665461   0.97418314]]\n",
      "18 1.1615098 [[ 0.0988362   1.8647875   0.21384555]\n",
      " [ 0.20769924 -0.6724312  -0.566278  ]\n",
      " [ 0.4704925   0.86781555  0.9616963 ]]\n",
      "19 1.1501739 [[ 0.09348346  1.8548323   0.22915347]\n",
      " [ 0.19910315 -0.669956   -0.56015706]\n",
      " [ 0.48159194  0.8690469   0.94936556]]\n",
      "20 1.13904 [[ 0.08815299  1.8449591   0.24435703]\n",
      " [ 0.19062044 -0.6674531  -0.5541773 ]\n",
      " [ 0.49257198  0.8702414   0.93719107]]\n",
      "21 1.128108 [[ 0.08284484  1.8351693   0.25945497]\n",
      " [ 0.18225184 -0.66492426 -0.5483375 ]\n",
      " [ 0.5034324   0.8713988   0.9251732 ]]\n",
      "22 1.1173761 [[ 0.0775589   1.8254641   0.27444604]\n",
      " [ 0.1739972  -0.6623705  -0.54263663]\n",
      " [ 0.5141723   0.8725199   0.9133122 ]]\n",
      "23 1.1068444 [[ 0.07229514  1.8158448   0.28932917]\n",
      " [ 0.16585697 -0.6597935  -0.53707343]\n",
      " [ 0.52479154  0.87360483  0.9016081 ]]\n",
      "24 1.0965111 [[ 0.06705343  1.8063123   0.30410329]\n",
      " [ 0.15783106 -0.65719444 -0.53164655]\n",
      " [ 0.53528935  0.8746542   0.8900609 ]]\n",
      "25 1.0863755 [[ 0.06183368  1.7968678   0.3187675 ]\n",
      " [ 0.14991963 -0.6545749  -0.52635473]\n",
      " [ 0.54566556  0.8756683   0.8786706 ]]\n",
      "26 1.0764358 [[ 0.05663574  1.7875124   0.33332092]\n",
      " [ 0.14212261 -0.6519361  -0.5211964 ]\n",
      " [ 0.55591977  0.87664783  0.8674369 ]]\n",
      "27 1.0666907 [[ 0.05145948  1.7782468   0.34776282]\n",
      " [ 0.13444003 -0.6492798  -0.5161702 ]\n",
      " [ 0.5660518   0.87759316  0.85635954]]\n",
      "28 1.0571386 [[ 0.04630471  1.7690718   0.36209258]\n",
      " [ 0.12687181 -0.6466072  -0.5112745 ]\n",
      " [ 0.57606137  0.878505    0.8454381 ]]\n",
      "29 1.0477774 [[ 0.04117125  1.7599882   0.37630963]\n",
      " [ 0.11941775 -0.64391994 -0.5065077 ]\n",
      " [ 0.5859484   0.87938386  0.8346722 ]]\n",
      "30 1.0386052 [[ 0.0360589   1.7509967   0.3904135 ]\n",
      " [ 0.11207771 -0.64121944 -0.5018681 ]\n",
      " [ 0.5957129   0.8802305   0.82406116]]\n",
      "31 1.0296202 [[ 0.03096743  1.7420979   0.4044038 ]\n",
      " [ 0.10485145 -0.6385072  -0.49735412]\n",
      " [ 0.6053548   0.88104546  0.8136043 ]]\n",
      "32 1.0208201 [[ 0.02589661  1.7332922   0.4182803 ]\n",
      " [ 0.09773869 -0.6357847  -0.49296385]\n",
      " [ 0.6148741   0.88182944  0.8033009 ]]\n",
      "33 1.0122026 [[ 0.02084619  1.7245802   0.43204275]\n",
      " [ 0.09073917 -0.6330534  -0.4886956 ]\n",
      " [ 0.62427115  0.8825832   0.7931501 ]]\n",
      "34 1.0037653 [[ 0.0158159   1.7159622   0.44569108]\n",
      " [ 0.08385246 -0.6303149  -0.4845474 ]\n",
      " [ 0.63354594  0.8833075   0.78315103]]\n",
      "35 0.9955059 [[ 0.01080548  1.7074385   0.45922524]\n",
      " [ 0.07707821 -0.6275705  -0.4805175 ]\n",
      " [ 0.64269876  0.8840031   0.77330256]]\n",
      "36 0.98742163 [[ 0.00581463  1.6990093   0.4726453 ]\n",
      " [ 0.07041596 -0.6248218  -0.47660396]\n",
      " [ 0.65172994  0.8846708   0.7636037 ]]\n",
      "37 0.97951 [[ 8.4306533e-04  1.6906748e+00  4.8595142e-01]\n",
      " [ 6.3865297e-02 -6.2207025e-01 -4.7280484e-01]\n",
      " [ 6.6063988e-01  8.8531131e-01  7.5405318e-01]]\n",
      "38 0.97176826 [[-0.00410954  1.682435    0.49914378]\n",
      " [ 0.05742555 -0.6193172  -0.46911818]\n",
      " [ 0.6694289   0.88592565  0.7446498 ]]\n",
      "39 0.9641936 [[-0.00904349  1.6742901   0.51222265]\n",
      " [ 0.05109628 -0.6165641  -0.465542  ]\n",
      " [ 0.6780975   0.88651454  0.73539233]]\n",
      "40 0.9567833 [[-0.0139591   1.6662399   0.52518845]\n",
      " [ 0.04487693 -0.61381245 -0.4620743 ]\n",
      " [ 0.6866462   0.8870788   0.7262793 ]]\n",
      "41 0.9495346 [[-0.01885672  1.6582844   0.53804153]\n",
      " [ 0.0387667  -0.6110634  -0.4587131 ]\n",
      " [ 0.6950756   0.8876195   0.71730936]]\n",
      "42 0.9424444 [[-0.02373664  1.6504234   0.55078244]\n",
      " [ 0.03276514 -0.60831857 -0.4554564 ]\n",
      " [ 0.70338625  0.8881372   0.70848095]]\n",
      "43 0.9355097 [[-0.02859924  1.6426568   0.56341165]\n",
      " [ 0.02687139 -0.605579   -0.4523022 ]\n",
      " [ 0.71157885  0.8886331   0.6997925 ]]\n",
      "44 0.9287279 [[-0.03344483  1.6349843   0.5759298 ]\n",
      " [ 0.02108482 -0.60284626 -0.4492484 ]\n",
      " [ 0.71965414  0.88910776  0.6912425 ]]\n",
      "45 0.9220957 [[-0.03827377  1.6274054   0.5883376 ]\n",
      " [ 0.01540455 -0.6001214  -0.44629303]\n",
      " [ 0.7276128   0.88956237  0.6828293 ]]\n",
      "46 0.9156104 [[-0.04308639  1.61992     0.60063565]\n",
      " [ 0.00982992 -0.5974057  -0.44343406]\n",
      " [ 0.7354557   0.8899976   0.6745512 ]]\n",
      "47 0.90926886 [[-0.04788304  1.6125276   0.61282474]\n",
      " [ 0.00435997 -0.59470034 -0.44066948]\n",
      " [ 0.74318355  0.8904144   0.66640645]]\n",
      "48 0.9030678 [[-5.2664045e-02  1.6052277e+00  6.2490565e-01]\n",
      " [-1.0060398e-03 -5.9200650e-01 -4.3799728e-01]\n",
      " [ 7.5079739e-01  8.9081365e-01  6.5839338e-01]]\n",
      "49 0.8970047 [[-0.05742976  1.5980198   0.63687927]\n",
      " [-0.00626909 -0.58932525 -0.43541548]\n",
      " [ 0.758298    0.89119625  0.6505102 ]]\n",
      "50 0.8910763 [[-0.06218051  1.5909034   0.64874643]\n",
      " [-0.01142997 -0.58665776 -0.43292212]\n",
      " [ 0.76568645  0.8915629   0.6427551 ]]\n",
      "51 0.88527983 [[-0.06691665  1.5838779   0.66050804]\n",
      " [-0.01648981 -0.5840048  -0.43051523]\n",
      " [ 0.7729635   0.89191467  0.63512623]]\n",
      "52 0.8796121 [[-0.07163846  1.5769428   0.67216504]\n",
      " [-0.0214493  -0.58136773 -0.42819282]\n",
      " [ 0.78013057  0.8922521   0.62762177]]\n",
      "53 0.87407017 [[-0.07634634  1.5700973   0.6837184 ]\n",
      " [-0.02630975 -0.5787471  -0.42595303]\n",
      " [ 0.78718823  0.89257634  0.62023985]]\n",
      "54 0.86865133 [[-0.08104052  1.5633408   0.69516903]\n",
      " [-0.03107183 -0.57614416 -0.42379388]\n",
      " [ 0.794138    0.89288783  0.61297864]]\n",
      "55 0.8633525 [[-0.08572139  1.5566726   0.70651805]\n",
      " [-0.03573693 -0.5735594  -0.42171356]\n",
      " [ 0.8009806   0.8931877   0.6058362 ]]\n",
      "56 0.858171 [[-0.0903892   1.550092    0.71776646]\n",
      " [-0.04030577 -0.57099396 -0.41971016]\n",
      " [ 0.8077175   0.89347637  0.5988107 ]]\n",
      "57 0.85310394 [[-0.09504429  1.5435982   0.7289153 ]\n",
      " [-0.04477964 -0.56844836 -0.41778186]\n",
      " [ 0.81434965  0.8937548   0.5919001 ]]\n",
      "58 0.84814847 [[-0.09968692  1.5371904   0.7399656 ]\n",
      " [-0.04915955 -0.5659235  -0.4159268 ]\n",
      " [ 0.82087827  0.89402354  0.5851027 ]]\n",
      "59 0.84330183 [[-0.1043174   1.530868    0.7509185 ]\n",
      " [-0.05344663 -0.56341994 -0.4141433 ]\n",
      " [ 0.8273046   0.8942835   0.57841647]]\n",
      "60 0.8385616 [[-0.10893598  1.5246301   0.7617751 ]\n",
      " [-0.05764196 -0.5609384  -0.41242948]\n",
      " [ 0.83362985  0.8945351   0.5718396 ]]\n",
      "61 0.83392465 [[-0.11354296  1.5184757   0.7725364 ]\n",
      " [-0.06174675 -0.5584794  -0.41078368]\n",
      " [ 0.83985525  0.8947792   0.5653701 ]]\n",
      "62 0.82938874 [[-0.11813859  1.5124041   0.7832036 ]\n",
      " [-0.0657621  -0.55604357 -0.40920419]\n",
      " [ 0.84598213  0.8950163   0.55900615]]\n",
      "63 0.824951 [[-0.12272312  1.5064144   0.7937778 ]\n",
      " [-0.06968927 -0.55363125 -0.4076893 ]\n",
      " [ 0.8520116   0.8952471   0.5527459 ]]\n",
      "64 0.8206092 [[-0.12729678  1.5005058   0.80426013]\n",
      " [-0.07352923 -0.5512432  -0.40623742]\n",
      " [ 0.85794526  0.89547193  0.54658735]]\n",
      "65 0.8163605 [[-0.13185984  1.4946774   0.81465167]\n",
      " [-0.07728352 -0.54887944 -0.4048469 ]\n",
      " [ 0.863784    0.89569175  0.54052883]]\n",
      "66 0.8122028 [[-0.13641249  1.4889282   0.8249535 ]\n",
      " [-0.08095298 -0.54654074 -0.40351617]\n",
      " [ 0.86952955  0.8959066   0.5345684 ]]\n",
      "67 0.8081336 [[-0.14095497  1.4832574   0.8351668 ]\n",
      " [-0.08453908 -0.5442272  -0.40224367]\n",
      " [ 0.8751829   0.8961174   0.5287043 ]]\n",
      "68 0.80415046 [[-0.14548749  1.4776641   0.8452927 ]\n",
      " [-0.0880429  -0.5419392  -0.40102786]\n",
      " [ 0.88074565  0.8963244   0.5229346 ]]\n",
      "69 0.8002515 [[-0.15001024  1.4721473   0.8553322 ]\n",
      " [-0.09146573 -0.53967696 -0.39986724]\n",
      " [ 0.88621897  0.8965281   0.5172576 ]]\n",
      "70 0.7964337 [[-0.15452342  1.4667062   0.8652865 ]\n",
      " [-0.09480873 -0.53744084 -0.39876035]\n",
      " [ 0.8916043   0.89672893  0.5116715 ]]\n",
      "71 0.79269564 [[-0.15902722  1.4613397   0.8751567 ]\n",
      " [-0.09807324 -0.53523093 -0.39770576]\n",
      " [ 0.89690286  0.8969273   0.50617456]]\n",
      "72 0.7890347 [[-0.1635218   1.456047    0.8849439 ]\n",
      " [-0.1012604  -0.5330475  -0.39670208]\n",
      " [ 0.9021162   0.8971235   0.500765  ]]\n",
      "73 0.78544915 [[-0.16800734  1.4508274   0.89464915]\n",
      " [-0.10437156 -0.5308905  -0.3957479 ]\n",
      " [ 0.90724546  0.89731807  0.4954412 ]]\n",
      "74 0.78193665 [[-0.172484    1.4456795   0.90427357]\n",
      " [-0.10740781 -0.52876025 -0.3948419 ]\n",
      " [ 0.9122922   0.8975111   0.49020138]]\n",
      "75 0.77849555 [[-0.17695193  1.4406029   0.9138182 ]\n",
      " [-0.11037056 -0.5266567  -0.39398277]\n",
      " [ 0.9172576   0.8977032   0.4850439 ]]\n",
      "76 0.7751235 [[-0.18141127  1.4355963   0.9232841 ]\n",
      " [-0.11326092 -0.52457994 -0.39316916]\n",
      " [ 0.92214316  0.8978944   0.47996712]]\n",
      "77 0.7718189 [[-0.18586215  1.4306589   0.9326724 ]\n",
      " [-0.1160802  -0.52252996 -0.39239988]\n",
      " [ 0.92695016  0.89808506  0.47496942]]\n",
      "78 0.76857996 [[-0.19030473  1.4257898   0.94198406]\n",
      " [-0.11882968 -0.5205067  -0.39167368]\n",
      " [ 0.93167996  0.89827555  0.47004914]]\n",
      "79 0.7654045 [[-0.19473909  1.4209881   0.95122015]\n",
      " [-0.12151045 -0.5185103  -0.39098933]\n",
      " [ 0.936334    0.8984659   0.4652048 ]]\n",
      "80 0.7622913 [[-0.19916539  1.4162529   0.9603817 ]\n",
      " [-0.12412395 -0.51654047 -0.39034566]\n",
      " [ 0.94091344  0.8986565   0.4604348 ]]\n",
      "81 0.75923836 [[-0.2035837   1.4115832   0.9694697 ]\n",
      " [-0.12667118 -0.51459736 -0.38974157]\n",
      " [ 0.9454198   0.89884734  0.4557376 ]]\n",
      "82 0.75624394 [[-0.20799416  1.4069781   0.9784852 ]\n",
      " [-0.12915362 -0.51268065 -0.38917586]\n",
      " [ 0.94985414  0.89903885  0.45111173]]\n",
      "83 0.7533068 [[-0.21239683  1.4024369   0.9874292 ]\n",
      " [-0.1315722  -0.51079047 -0.3886475 ]\n",
      " [ 0.9542181   0.8992309   0.44655567]]\n",
      "84 0.75042504 [[-0.21679185  1.3979585   0.9963026 ]\n",
      " [-0.13392842 -0.5089264  -0.38815537]\n",
      " [ 0.9585127   0.89942396  0.44206798]]\n",
      "85 0.74759716 [[-0.22117925  1.3935422   1.0051064 ]\n",
      " [-0.1362232  -0.50708854 -0.38769844]\n",
      " [ 0.9627395   0.8996178   0.43764728]]\n",
      "86 0.74482214 [[-0.22555915  1.389187    1.0138416 ]\n",
      " [-0.13845795 -0.5052765  -0.3872757 ]\n",
      " [ 0.96689963  0.8998129   0.43329212]]\n",
      "87 0.7420979 [[-0.2299316   1.384892    1.0225091 ]\n",
      " [-0.1406337  -0.5034903  -0.38688612]\n",
      " [ 0.9709945   0.90000904  0.42900115]]\n",
      "88 0.7394234 [[-0.2342967   1.3806565   1.0311097 ]\n",
      " [-0.14275181 -0.50172955 -0.38652876]\n",
      " [ 0.97502506  0.9002066   0.42477298]]\n",
      "89 0.7367972 [[-0.23865446  1.3764795   1.0396445 ]\n",
      " [-0.14481317 -0.4999943  -0.38620266]\n",
      " [ 0.978993    0.90040535  0.4206063 ]]\n",
      "90 0.7342181 [[-0.24300504  1.3723602   1.0481143 ]\n",
      " [-0.14681932 -0.49828392 -0.3859069 ]\n",
      " [ 0.98289907  0.9006058   0.41649976]]\n",
      "91 0.73168474 [[-0.24734837  1.3682978   1.05652   ]\n",
      " [-0.1487709  -0.4965987  -0.38564056]\n",
      " [ 0.98674506  0.9008074   0.4124522 ]]\n",
      "92 0.72919583 [[-0.2516846   1.3642915   1.0648625 ]\n",
      " [-0.15066956 -0.49493778 -0.38540277]\n",
      " [ 0.9905316   0.90101075  0.40846226]]\n",
      "93 0.7267505 [[-0.25601372  1.3603405   1.0731426 ]\n",
      " [-0.15251596 -0.49330145 -0.38519266]\n",
      " [ 0.99426043  0.90121543  0.40452874]]\n",
      "94 0.724347 [[-0.26033583  1.3564439   1.0813613 ]\n",
      " [-0.1543116  -0.49168906 -0.3850094 ]\n",
      " [ 0.99793226  0.9014219   0.40065044]]\n",
      "95 0.7219846 [[-0.26465094  1.3526009   1.0895193 ]\n",
      " [-0.15605721 -0.49010065 -0.3848522 ]\n",
      " [ 1.0015486   0.90162975  0.39682618]]\n",
      "96 0.7196623 [[-0.2689591   1.3488109   1.0976175 ]\n",
      " [-0.15775421 -0.4885356  -0.38472024]\n",
      " [ 1.0051104   0.9018394   0.39305478]]\n",
      "97 0.7173788 [[-0.27326033  1.345073    1.1056566 ]\n",
      " [-0.15940335 -0.48699397 -0.38461274]\n",
      " [ 1.008619    0.9020505   0.3893351 ]]\n",
      "98 0.7151331 [[-0.27755466  1.3413863   1.1136376 ]\n",
      " [-0.16100593 -0.4854752  -0.38452893]\n",
      " [ 1.0120752   0.9022632   0.38566607]]\n",
      "99 0.71292436 [[-0.28184214  1.3377503   1.121561  ]\n",
      " [-0.16256283 -0.48397917 -0.38446808]\n",
      " [ 1.0154804   0.9024775   0.38204658]]\n",
      "100 0.7107514 [[-0.2861228   1.3341641   1.1294279 ]\n",
      " [-0.16407515 -0.4825054  -0.38442948]\n",
      " [ 1.0188355   0.9026934   0.37847555]]\n",
      "101 0.7086134 [[-0.29039663  1.330627    1.1372389 ]\n",
      " [-0.16554384 -0.4810538  -0.38441244]\n",
      " [ 1.0221418   0.9029107   0.37495193]]\n",
      "102 0.7065095 [[-0.2946637   1.3271382   1.1449947 ]\n",
      " [-0.16697    -0.47962385 -0.38441625]\n",
      " [ 1.0254002   0.9031296   0.3714747 ]]\n",
      "103 0.7044385 [[-0.298924    1.3236971   1.1526961 ]\n",
      " [-0.16835448 -0.47821537 -0.38444027]\n",
      " [ 1.0286117   0.9033499   0.3680429 ]]\n",
      "104 0.70239997 [[-0.3031776   1.320303    1.1603439 ]\n",
      " [-0.16969834 -0.47682795 -0.3844838 ]\n",
      " [ 1.0317774   0.9035716   0.3646555 ]]\n",
      "105 0.70039296 [[-0.30742446  1.3169551   1.1679387 ]\n",
      " [-0.17100243 -0.4754614  -0.38454628]\n",
      " [ 1.0348983   0.90379465  0.36131155]]\n",
      "106 0.6984166 [[-0.3116646   1.3136528   1.1754812 ]\n",
      " [-0.17226778 -0.47411528 -0.38462704]\n",
      " [ 1.0379753   0.9040191   0.3580101 ]]\n",
      "107 0.6964701 [[-0.31589806  1.3103952   1.1829722 ]\n",
      " [-0.1734952  -0.47278938 -0.38472548]\n",
      " [ 1.0410095   0.9042448   0.35475028]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 0.69455266 [[-0.32012486  1.307182    1.1904123 ]\n",
      " [-0.1746857  -0.4714833  -0.38484105]\n",
      " [ 1.0440018   0.9044717   0.35153112]]\n",
      "109 0.6926638 [[-0.324345    1.3040123   1.1978022 ]\n",
      " [-0.17584011 -0.47019678 -0.38497317]\n",
      " [ 1.0469531   0.9046998   0.34835178]]\n",
      "110 0.6908026 [[-0.32855847  1.3008854   1.2051425 ]\n",
      " [-0.17695926 -0.46892953 -0.38512126]\n",
      " [ 1.0498643   0.9049289   0.3452114 ]]\n",
      "111 0.6889683 [[-0.3327653   1.2978009   1.2124339 ]\n",
      " [-0.17804407 -0.46768117 -0.3852848 ]\n",
      " [ 1.0527364   0.9051592   0.34210908]]\n",
      "112 0.6871604 [[-0.33696553  1.294758    1.2196771 ]\n",
      " [-0.17909533 -0.4664514  -0.3854633 ]\n",
      " [ 1.0555702   0.9053904   0.33904406]]\n",
      "113 0.68537825 [[-0.34115914  1.2917562   1.2268726 ]\n",
      " [-0.18011391 -0.4652399  -0.3856562 ]\n",
      " [ 1.0583667   0.90562254  0.33601552]]\n",
      "114 0.683621 [[-0.34534612  1.2887948   1.234021  ]\n",
      " [-0.18110056 -0.46404648 -0.38586298]\n",
      " [ 1.0611266   0.9058555   0.3330227 ]]\n",
      "115 0.68188846 [[-0.34952652  1.2858732   1.241123  ]\n",
      " [-0.18205617 -0.46287063 -0.38608322]\n",
      " [ 1.0638506   0.9060893   0.3300648 ]]\n",
      "116 0.6801795 [[-0.3537003   1.2829908   1.2481792 ]\n",
      " [-0.18298134 -0.4617123  -0.3863164 ]\n",
      " [ 1.06654     0.9063237   0.32714108]]\n",
      "117 0.6784941 [[-0.35786754  1.2801471   1.2551901 ]\n",
      " [-0.18387707 -0.46057087 -0.3865621 ]\n",
      " [ 1.0691952   0.9065589   0.32425073]]\n",
      "118 0.6768313 [[-0.36202815  1.2773414   1.2621564 ]\n",
      " [-0.18474388 -0.4594463  -0.38681984]\n",
      " [ 1.071817    0.90679455  0.32139313]]\n",
      "119 0.6751908 [[-0.3661822   1.2745732   1.2690786 ]\n",
      " [-0.18558265 -0.45833817 -0.3870892 ]\n",
      " [ 1.0744064   0.9070308   0.31856754]]\n",
      "120 0.673572 [[-0.37032968  1.271842    1.2759572 ]\n",
      " [-0.186394   -0.45724627 -0.38736975]\n",
      " [ 1.076964    0.9072675   0.31577328]]\n",
      "121 0.6719743 [[-0.3744706   1.2691473   1.2827929 ]\n",
      " [-0.18717875 -0.4561702  -0.38766107]\n",
      " [ 1.0794905   0.9075046   0.31300968]]\n",
      "122 0.6703974 [[-0.37860492  1.2664884   1.2895862 ]\n",
      " [-0.18793738 -0.45510983 -0.3879628 ]\n",
      " [ 1.0819869   0.9077419   0.31027606]]\n",
      "123 0.66884065 [[-0.38273275  1.263865    1.2963375 ]\n",
      " [-0.18867089 -0.4540646  -0.3882745 ]\n",
      " [ 1.0844535   0.90797967  0.30757177]]\n",
      "124 0.66730374 [[-0.386854    1.2612764   1.3030474 ]\n",
      " [-0.18937954 -0.45303464 -0.3885958 ]\n",
      " [ 1.0868913   0.9082174   0.30489624]]\n",
      "125 0.665786 [[-0.3909687   1.2587221   1.3097165 ]\n",
      " [-0.19006439 -0.45201924 -0.38892636]\n",
      " [ 1.0893006   0.90845543  0.30224878]]\n",
      "126 0.6642873 [[-0.39507687  1.2562015   1.3163451 ]\n",
      " [-0.19072562 -0.45101857 -0.38926578]\n",
      " [ 1.0916827   0.9086933   0.29962885]]\n",
      "127 0.66280687 [[-0.39917853  1.2537143   1.3229339 ]\n",
      " [-0.19136429 -0.4500319  -0.38961378]\n",
      " [ 1.0940377   0.9089314   0.29703584]]\n",
      "128 0.6613447 [[-0.40327364  1.25126     1.3294833 ]\n",
      " [-0.19198069 -0.44905937 -0.38996994]\n",
      " [ 1.0963664   0.9091692   0.2944692 ]]\n",
      "129 0.6599001 [[-0.40736225  1.2488382   1.3359938 ]\n",
      " [-0.1925756  -0.44810042 -0.39033398]\n",
      " [ 1.0986694   0.909407    0.29192835]]\n",
      "130 0.6584728 [[-0.4114443   1.2464482   1.3424659 ]\n",
      " [-0.19314942 -0.44715503 -0.39070556]\n",
      " [ 1.1009475   0.9096445   0.28941274]]\n",
      "131 0.6570623 [[-0.4155199   1.2440896   1.3489    ]\n",
      " [-0.19370292 -0.4462227  -0.39108437]\n",
      " [ 1.103201    0.90988183  0.28692183]]\n",
      "132 0.65566844 [[-0.41958895  1.241762    1.3552966 ]\n",
      " [-0.19423644 -0.4453034  -0.39147013]\n",
      " [ 1.1054308   0.91011876  0.28445512]]\n",
      "133 0.6542907 [[-0.42365155  1.2394651   1.3616562 ]\n",
      " [-0.19475076 -0.44439673 -0.3918625 ]\n",
      " [ 1.1076372   0.9103554   0.2820121 ]]\n",
      "134 0.6529288 [[-0.42770764  1.2371982   1.3679792 ]\n",
      " [-0.19524616 -0.4435026  -0.3922612 ]\n",
      " [ 1.1098208   0.9105915   0.2795923 ]]\n",
      "135 0.65158224 [[-0.43175724  1.234961    1.3742659 ]\n",
      " [-0.1957233  -0.44262066 -0.392666  ]\n",
      " [ 1.1119823   0.91082716  0.2771952 ]]\n",
      "136 0.65025115 [[-0.43580037  1.2327532   1.380517  ]\n",
      " [-0.19618264 -0.44175074 -0.3930766 ]\n",
      " [ 1.1141222   0.91106224  0.2748203 ]]\n",
      "137 0.6489347 [[-0.43983704  1.2305741   1.3867327 ]\n",
      " [-0.19662468 -0.44089258 -0.39349273]\n",
      " [ 1.1162407   0.9112967   0.27246717]]\n",
      "138 0.64763284 [[-0.44386724  1.2284235   1.3929136 ]\n",
      " [-0.1970499  -0.44004598 -0.39391413]\n",
      " [ 1.1183387   0.91153055  0.27013537]]\n",
      "139 0.64634526 [[-0.447891    1.226301    1.3990599 ]\n",
      " [-0.19745876 -0.43921068 -0.39434057]\n",
      " [ 1.1204165   0.91176367  0.2678244 ]]\n",
      "140 0.64507186 [[-0.45190832  1.2242061   1.4051721 ]\n",
      " [-0.19785173 -0.4383865  -0.39477178]\n",
      " [ 1.1224747   0.91199607  0.26553392]]\n",
      "141 0.6438118 [[-0.4559192   1.2221384   1.4112507 ]\n",
      " [-0.19822925 -0.4375732  -0.39520755]\n",
      " [ 1.1245136   0.91222763  0.26326343]]\n",
      "142 0.64256537 [[-0.45992365  1.2200977   1.4172959 ]\n",
      " [-0.19859175 -0.43677062 -0.39564764]\n",
      " [ 1.1265337   0.9124583   0.26101255]]\n",
      "143 0.6413319 [[-0.4639217   1.2180834   1.4233083 ]\n",
      " [-0.19893964 -0.43597853 -0.39609185]\n",
      " [ 1.1285356   0.9126881   0.2587809 ]]\n",
      "144 0.6401115 [[-0.46791336  1.2160952   1.429288  ]\n",
      " [-0.1992734  -0.43519664 -0.39653996]\n",
      " [ 1.1305196   0.912917    0.256568  ]]\n",
      "145 0.63890374 [[-0.47189862  1.2141328   1.4352356 ]\n",
      " [-0.19959329 -0.434425   -0.39699173]\n",
      " [ 1.1324862   0.9131448   0.2543736 ]]\n",
      "146 0.6377082 [[-0.4758775   1.2121959   1.4411515 ]\n",
      " [-0.19989988 -0.43366313 -0.39744702]\n",
      " [ 1.1344358   0.9133717   0.2521972 ]]\n",
      "147 0.6365249 [[-0.47985     1.210284    1.4470359 ]\n",
      " [-0.20019343 -0.432911   -0.39790556]\n",
      " [ 1.1363688   0.91359746  0.2500385 ]]\n",
      "148 0.63535345 [[-0.48381612  1.2083968   1.4528892 ]\n",
      " [-0.2004744  -0.43216842 -0.3983672 ]\n",
      " [ 1.1382854   0.9138221   0.24789715]]\n",
      "149 0.63419396 [[-0.4877759   1.206534    1.4587117 ]\n",
      " [-0.200743   -0.43143523 -0.39883175]\n",
      " [ 1.1401863   0.9140456   0.24577276]]\n",
      "150 0.6330457 [[-0.49172935  1.2046952   1.464504  ]\n",
      " [-0.20099983 -0.4307111  -0.39929906]\n",
      " [ 1.1420717   0.91426796  0.243665  ]]\n",
      "151 0.6319086 [[-0.49567646  1.2028801   1.4702662 ]\n",
      " [-0.201245   -0.42999607 -0.39976892]\n",
      " [ 1.1439421   0.914489    0.24157354]]\n",
      "152 0.6307826 [[-0.49961725  1.2010885   1.4759986 ]\n",
      " [-0.201479   -0.4292898  -0.40024117]\n",
      " [ 1.1457977   0.9147088   0.23949805]]\n",
      "153 0.6296676 [[-0.5035517   1.19932     1.4817017 ]\n",
      " [-0.2017021  -0.4285922  -0.40071565]\n",
      " [ 1.147639    0.9149273   0.23743823]]\n",
      "154 0.62856317 [[-0.50747997  1.1975741   1.4873757 ]\n",
      " [-0.20191471 -0.42790306 -0.4011922 ]\n",
      " [ 1.1494663   0.91514456  0.23539376]]\n",
      "155 0.6274692 [[-0.5114019   1.1958508   1.493021  ]\n",
      " [-0.202117   -0.42722234 -0.40167063]\n",
      " [ 1.1512799   0.9153604   0.23336436]]\n",
      "156 0.62638533 [[-0.51531756  1.1941497   1.4986379 ]\n",
      " [-0.2023094  -0.4265497  -0.40215084]\n",
      " [ 1.1530801   0.9155749   0.23134969]]\n",
      "157 0.6253116 [[-0.51922697  1.1924704   1.5042267 ]\n",
      " [-0.20249209 -0.42588517 -0.40263265]\n",
      " [ 1.1548673   0.91578794  0.2293495 ]]\n",
      "158 0.6242479 [[-0.5231302   1.1908127   1.5097877 ]\n",
      " [-0.20266546 -0.42522848 -0.40311596]\n",
      " [ 1.1566417   0.9159996   0.22736345]]\n",
      "159 0.62319386 [[-0.5270272   1.1891763   1.5153211 ]\n",
      " [-0.20282972 -0.42457956 -0.40360063]\n",
      " [ 1.1584038   0.9162097   0.22539133]]\n",
      "160 0.6221493 [[-0.530918    1.1875609   1.5208274 ]\n",
      " [-0.20298524 -0.42393816 -0.4040865 ]\n",
      " [ 1.1601535   0.91641843  0.22343282]]\n",
      "161 0.62111413 [[-0.5348026   1.1859663   1.5263067 ]\n",
      " [-0.20313212 -0.42330432 -0.40457344]\n",
      " [ 1.1618916   0.9166255   0.2214877 ]]\n",
      "162 0.6200883 [[-0.5386811   1.1843921   1.5317594 ]\n",
      " [-0.2032709  -0.4226776  -0.40506136]\n",
      " [ 1.1636178   0.91683125  0.21955568]]\n",
      "163 0.61907154 [[-0.54255337  1.182838    1.5371857 ]\n",
      " [-0.20340142 -0.42205834 -0.4055501 ]\n",
      " [ 1.165333    0.91703516  0.21763654]]\n",
      "164 0.61806345 [[-0.5464195   1.1813039   1.542586  ]\n",
      " [-0.20352441 -0.42144585 -0.4060396 ]\n",
      " [ 1.167037    0.91723776  0.21573   ]]\n",
      "165 0.61706424 [[-0.5502795   1.1797894   1.5479604 ]\n",
      " [-0.2036397  -0.42084047 -0.4065297 ]\n",
      " [ 1.1687304   0.91743857  0.21383584]]\n",
      "166 0.6160737 [[-0.5541334   1.1782944   1.5533093 ]\n",
      " [-0.20374781 -0.42024174 -0.4070203 ]\n",
      " [ 1.1704131   0.9176379   0.21195382]]\n",
      "167 0.6150915 [[-0.55798125  1.1768186   1.558633  ]\n",
      " [-0.20384881 -0.41964975 -0.4075113 ]\n",
      " [ 1.1720856   0.91783553  0.21008372]]\n",
      "168 0.61411774 [[-0.561823    1.1753616   1.5639317 ]\n",
      " [-0.20394298 -0.41906428 -0.4080026 ]\n",
      " [ 1.173748    0.9180315   0.20822534]]\n",
      "169 0.61315215 [[-0.5656587   1.1739234   1.5692056 ]\n",
      " [-0.2040305  -0.41848528 -0.40849409]\n",
      " [ 1.1754006   0.9182258   0.2063784 ]]\n",
      "170 0.61219466 [[-0.56948835  1.1725036   1.5744551 ]\n",
      " [-0.20411159 -0.41791254 -0.4089857 ]\n",
      " [ 1.1770437   0.91841847  0.20454273]]\n",
      "171 0.61124486 [[-0.573312    1.1711019   1.5796804 ]\n",
      " [-0.20418645 -0.41734606 -0.40947732]\n",
      " [ 1.1786774   0.9186094   0.20271811]]\n",
      "172 0.61030304 [[-0.57712966  1.1697183   1.5848818 ]\n",
      " [-0.2042554  -0.41678557 -0.40996888]\n",
      " [ 1.1803019   0.9187987   0.20090434]]\n",
      "173 0.60936886 [[-0.5809413   1.1683524   1.5900594 ]\n",
      " [-0.20431837 -0.41623122 -0.41046026]\n",
      " [ 1.1819177   0.9189861   0.19910121]]\n",
      "174 0.60844207 [[-0.584747    1.167004    1.5952135 ]\n",
      " [-0.20437594 -0.4156825  -0.4109514 ]\n",
      " [ 1.1835245   0.919172    0.19730854]]\n",
      "175 0.6075229 [[-0.58854675  1.1656729   1.6003444 ]\n",
      " [-0.20442784 -0.4151398  -0.41144222]\n",
      " [ 1.185123    0.9193559   0.19552614]]\n",
      "176 0.606611 [[-0.5923406   1.1643589   1.6054523 ]\n",
      " [-0.20447458 -0.4146026  -0.41193265]\n",
      " [ 1.186713    0.91953826  0.1937538 ]]\n",
      "177 0.6057062 [[-0.5961285   1.1630617   1.6105374 ]\n",
      " [-0.20451619 -0.41407105 -0.4124226 ]\n",
      " [ 1.1882949   0.91971874  0.19199137]]\n",
      "178 0.60480857 [[-0.59991056  1.1617813   1.6155999 ]\n",
      " [-0.20455284 -0.41354495 -0.412912  ]\n",
      " [ 1.1898689   0.9198975   0.19023865]]\n",
      "179 0.60391784 [[-0.60368675  1.1605173   1.62064   ]\n",
      " [-0.20458482 -0.4130242  -0.4134008 ]\n",
      " [ 1.1914351   0.9200745   0.1884955 ]]\n",
      "180 0.6030339 [[-0.60745704  1.1592696   1.6256582 ]\n",
      " [-0.20461209 -0.41250885 -0.41388887]\n",
      " [ 1.1929938   0.92024964  0.18676175]]\n",
      "181 0.6021569 [[-0.61122155  1.1580379   1.6306543 ]\n",
      " [-0.20463501 -0.41199854 -0.41437623]\n",
      " [ 1.1945449   0.9204231   0.1850372 ]]\n",
      "182 0.6012864 [[-0.6149802   1.1568221   1.6356288 ]\n",
      " [-0.20465356 -0.41149345 -0.41486275]\n",
      " [ 1.1960889   0.92059463  0.1833217 ]]\n",
      "183 0.6004225 [[-0.6187331   1.1556219   1.6405818 ]\n",
      " [-0.20466805 -0.4109933  -0.4153484 ]\n",
      " [ 1.1976256   0.92076445  0.1816151 ]]\n",
      "184 0.5995652 [[-0.6224802   1.1544372   1.6455137 ]\n",
      " [-0.20467839 -0.4104982  -0.41583315]\n",
      " [ 1.1991556   0.92093235  0.17991725]]\n",
      "185 0.59871405 [[-0.6262216   1.1532677   1.6504245 ]\n",
      " [-0.204685   -0.4100078  -0.4163169 ]\n",
      " [ 1.2006786   0.9210986   0.17822798]]\n",
      "186 0.5978692 [[-0.6299572   1.1521133   1.6553144 ]\n",
      " [-0.20468774 -0.40952235 -0.4167996 ]\n",
      " [ 1.2021952   0.92126286  0.17654717]]\n",
      "187 0.59703064 [[-0.63368714  1.1509739   1.6601838 ]\n",
      " [-0.20468707 -0.4090414  -0.41728124]\n",
      " [ 1.2037051   0.92142546  0.17487465]]\n",
      "188 0.59619796 [[-0.63741136  1.1498492   1.6650327 ]\n",
      " [-0.20468275 -0.40856522 -0.41776174]\n",
      " [ 1.2052088   0.9215861   0.17321028]]\n",
      "189 0.59537137 [[-0.6411299   1.1487391   1.6698614 ]\n",
      " [-0.20467518 -0.40809345 -0.41824105]\n",
      " [ 1.2067062   0.921745    0.17155394]]\n",
      "190 0.5945506 [[-0.6448428   1.1476433   1.6746701 ]\n",
      " [-0.20466433 -0.4076262  -0.4187191 ]\n",
      " [ 1.2081976   0.921902    0.1699055 ]]\n",
      "191 0.5937358 [[-0.64855003  1.1465619   1.6794589 ]\n",
      " [-0.20465043 -0.4071633  -0.41919592]\n",
      " [ 1.2096831   0.9220573   0.1682648 ]]\n",
      "192 0.5929267 [[-0.65225166  1.1454943   1.684228  ]\n",
      " [-0.20463346 -0.40670478 -0.4196714 ]\n",
      " [ 1.2111628   0.92221063  0.16663176]]\n",
      "193 0.59212315 [[-0.65594774  1.1444408   1.6889776 ]\n",
      " [-0.20461372 -0.40625042 -0.42014548]\n",
      " [ 1.2126367   0.92236227  0.16500622]]\n",
      "194 0.5913253 [[-0.6596382   1.1434009   1.693708  ]\n",
      " [-0.20459107 -0.40580034 -0.42061818]\n",
      " [ 1.2141051   0.922512    0.16338807]]\n",
      "195 0.5905329 [[-0.66332316  1.1423746   1.6984192 ]\n",
      " [-0.20456578 -0.40535435 -0.42108947]\n",
      " [ 1.2155681   0.9226599   0.16177717]]\n",
      "196 0.5897459 [[-0.66700256  1.1413617   1.7031115 ]\n",
      " [-0.20453794 -0.40491238 -0.42155927]\n",
      " [ 1.2170258   0.92280596  0.16017342]]\n",
      "197 0.58896416 [[-0.67067647  1.140362    1.7077851 ]\n",
      " [-0.20450763 -0.40447438 -0.4220276 ]\n",
      " [ 1.2184782   0.92295027  0.1585767 ]]\n",
      "198 0.5881878 [[-0.6743449   1.1393754   1.7124401 ]\n",
      " [-0.20447496 -0.4040403  -0.42249432]\n",
      " [ 1.2199255   0.9230927   0.1569869 ]]\n",
      "199 0.5874165 [[-0.67800784  1.1384017   1.7170767 ]\n",
      " [-0.20443995 -0.40361014 -0.4229595 ]\n",
      " [ 1.2213678   0.92323333  0.15540391]]\n",
      "200 0.5866505 [[-0.68166536  1.1374409   1.721695  ]\n",
      " [-0.20440277 -0.40318373 -0.42342308]\n",
      " [ 1.2228053   0.92337215  0.15382762]]\n",
      "Prediction:  [2 2 2]\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "nb_classes = 3\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "# model / hypothesis\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Correct prediction Test model\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "        \n",
    "    # predict\n",
    "    print(\"Prediction: \", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 1], [0, 0, 1], [0, 0, 1]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test  # prediction 모두 클래스 2 => 정확도 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예제2. Learning rate 실습 : learning_rate = 1.5 로 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.095914 [[-1.1047618   0.76345074 -0.18344778]\n",
      " [-3.5743513   3.1056266   2.2014568 ]\n",
      " [-3.3643415   2.9881532  -0.9072043 ]]\n",
      "1 inf [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "2 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "3 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "4 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "5 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "6 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "7 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "8 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "9 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "10 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "11 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "12 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "13 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "14 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "15 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "16 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "17 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "18 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "19 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "20 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "21 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "22 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "23 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "24 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "25 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "26 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "27 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "28 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "29 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "30 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "31 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "32 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "33 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "34 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "35 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "36 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "37 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "38 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "39 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "40 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "41 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "42 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "43 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "44 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "45 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "46 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "47 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "48 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "49 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "50 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "51 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "52 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "53 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "54 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "55 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "56 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "57 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "58 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "59 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "60 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "61 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "62 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "63 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "64 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "65 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "66 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "67 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "68 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "69 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "70 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "71 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "72 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "73 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "74 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "75 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "76 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "77 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "78 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "79 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "80 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "81 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "82 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "83 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "84 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "85 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "86 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "87 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "88 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "89 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "90 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "91 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "92 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "93 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "94 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "95 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "96 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "97 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "98 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "99 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "100 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "101 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "102 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "103 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "104 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "105 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "106 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "107 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "108 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "109 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "110 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "111 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "112 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "113 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "114 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "115 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "116 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "117 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "118 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "119 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "120 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "121 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "122 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "123 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "124 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "125 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "126 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "127 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "128 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "129 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "130 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "131 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "132 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "133 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "134 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "135 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "136 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "137 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "138 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "139 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "140 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "141 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "142 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "143 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "144 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "145 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "146 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "147 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "148 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "149 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "150 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "151 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "152 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "153 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "154 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "155 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "156 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "157 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "158 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "159 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "160 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "161 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "162 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "163 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "164 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "165 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "166 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "167 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "168 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "169 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "170 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "171 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "172 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "173 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "174 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "175 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "176 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "177 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "178 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "179 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "180 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "181 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "182 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "183 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "184 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "185 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "186 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "187 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "188 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "189 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "190 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "191 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "192 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "193 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "194 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "195 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "196 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "197 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "198 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "199 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "200 nan [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "Prediction:  [0 0 0]\n",
      "Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "nb_classes = 3\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "# model / hypothesis\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.5).minimize(cost)\n",
    "\n",
    "# Correct prediction Test model\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "        \n",
    "    # predict\n",
    "    print(\"Prediction: \", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cost 함수 값이 발산하고 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 반대로 learning rate를 작게 하면.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "1 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "2 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "3 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "4 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "5 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "6 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "7 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "8 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "9 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "10 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "11 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "12 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "13 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "14 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "15 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "16 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "17 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "18 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "19 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "20 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "21 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "22 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "23 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "24 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "25 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "26 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "27 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "28 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "29 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "30 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "31 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "32 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "33 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "34 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "35 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "36 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "37 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "38 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "39 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "40 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "41 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "42 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "43 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "44 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "45 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "46 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "47 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "48 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "49 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "50 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "51 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "52 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "53 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "54 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "55 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "56 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "57 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "58 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "59 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "60 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "61 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "62 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "63 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "64 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "65 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "66 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "67 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "68 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "69 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "70 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "71 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "72 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "73 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "74 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "75 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "76 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "77 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "78 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "79 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "80 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "81 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "82 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "83 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "84 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "85 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "86 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "87 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "88 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "89 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "90 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "91 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "92 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "93 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "94 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "95 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "96 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "97 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "98 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "99 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "100 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "101 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "102 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "103 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "104 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "105 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "106 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "107 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "108 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "109 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "110 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "111 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "112 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "113 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "114 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "115 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "116 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "117 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "118 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "119 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "120 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "121 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "122 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "123 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "124 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "125 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "126 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "127 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "128 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "129 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "130 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "131 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "132 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "133 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "134 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "135 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "136 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "137 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "138 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "139 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "140 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "141 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "142 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "143 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "145 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "146 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "147 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "148 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "149 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "150 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "151 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "152 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "153 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "154 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "155 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "156 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "157 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "158 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "159 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "160 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "161 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "162 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "163 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "164 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "165 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "166 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "167 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "168 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "169 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "170 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "171 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "172 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "173 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "174 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "175 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "176 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "177 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "178 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "179 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "180 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "181 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "182 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "183 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "184 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "185 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "186 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "187 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "188 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "189 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "190 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "191 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "192 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "193 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "194 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "195 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "196 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "197 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "198 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "199 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "200 8.091789 [[ 1.1780343   1.222602    1.1322732 ]\n",
      " [ 0.41207463 -1.4281347   0.20146571]\n",
      " [ 1.1556472   0.16836    -1.6881446 ]]\n",
      "Prediction:  [0 0 0]\n",
      "Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "nb_classes = 3\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "# model / hypothesis\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-10).minimize(cost)\n",
    "\n",
    "# Correct prediction Test model\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "        \n",
    "    # predict\n",
    "    print(\"Prediction: \", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cost 함수가 줄어들지 않아 트레이닝이 안됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예제3. Non-normalized inputs(정규화되지 않은 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = xy[:, :-1]\n",
    "y_data = xy[:, [-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8, 4), (8, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape, y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost: 756695040000.0 \n",
      "Prediction:\n",
      " [[ -613367.56]\n",
      " [-1233735.8 ]\n",
      " [ -970746.75]\n",
      " [ -680784.1 ]\n",
      " [ -802165.6 ]\n",
      " [ -808908.06]\n",
      " [ -741467.2 ]\n",
      " [ -943760.7 ]]\n",
      "1 Cost: 8.313657e+26 \n",
      "Prediction:\n",
      " [[2.0338895e+13]\n",
      " [4.0944280e+13]\n",
      " [3.2209390e+13]\n",
      " [2.2578610e+13]\n",
      " [2.6610099e+13]\n",
      " [2.6834071e+13]\n",
      " [2.4594354e+13]\n",
      " [3.1313503e+13]]\n",
      "2 Cost: inf \n",
      "Prediction:\n",
      " [[-6.74160083e+20]\n",
      " [-1.35715341e+21]\n",
      " [-1.06762368e+21]\n",
      " [-7.48398546e+20]\n",
      " [-8.82027665e+20]\n",
      " [-8.89451497e+20]\n",
      " [-8.15213105e+20]\n",
      " [-1.03792835e+21]]\n",
      "3 Cost: inf \n",
      "Prediction:\n",
      " [[2.2345945e+28]\n",
      " [4.4984678e+28]\n",
      " [3.5387823e+28]\n",
      " [2.4806676e+28]\n",
      " [2.9235994e+28]\n",
      " [2.9482067e+28]\n",
      " [2.7021336e+28]\n",
      " [3.4403531e+28]]\n",
      "4 Cost: inf \n",
      "Prediction:\n",
      " [[-7.40686582e+35]\n",
      " [-1.49107814e+36]\n",
      " [-1.17297730e+36]\n",
      " [-8.22250787e+35]\n",
      " [-9.69066593e+35]\n",
      " [-9.77223053e+35]\n",
      " [-8.95658690e+35]\n",
      " [-1.14035154e+36]]\n",
      "5 Cost: inf \n",
      "Prediction:\n",
      " [[inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]\n",
      " [inf]]\n",
      "6 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "7 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "8 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "9 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "10 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "11 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "12 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "13 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "14 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "15 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "16 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "17 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "18 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "19 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "20 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "21 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "22 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "23 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "24 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "25 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "26 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "27 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "28 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "29 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "30 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "31 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "32 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "33 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "34 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "35 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "36 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "37 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "38 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "39 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "40 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "41 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "42 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "43 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "44 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "45 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "46 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "47 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "48 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "49 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "50 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "51 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "52 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "53 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "54 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "55 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "56 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "57 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "58 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "59 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "60 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "61 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "62 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "63 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "64 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "65 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "66 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "67 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "68 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "69 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "70 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "71 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "72 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "73 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "74 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "75 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "76 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "77 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "78 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "79 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "80 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "81 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "82 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "83 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "84 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "85 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "86 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "87 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "88 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "89 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "90 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "91 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "92 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "93 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "94 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "95 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "96 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "97 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "98 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "99 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "100 Cost: nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# model / hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-5).minimize(cost)\n",
    "\n",
    "# Launch the graph\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(101):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    print(step, \"Cost:\", cost_val, \"\\nPrediction:\\n\", hy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cost 함수 값이 수렴되지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터를 정규화(min-max scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9999999  0.9999999  0.         0.9999999  0.9999999 ]\n",
      " [0.70548484 0.70439545 0.9999999  0.71881775 0.83755783]\n",
      " [0.54412544 0.50274819 0.5760869  0.60646794 0.66063303]\n",
      " [0.33890349 0.3136802  0.10869564 0.4598913  0.43800914]\n",
      " [0.51435995 0.42582385 0.3043478  0.585048   0.42624397]\n",
      " [0.49556174 0.42582385 0.31521736 0.48131129 0.49276132]\n",
      " [0.11436063 0.         0.20652172 0.22007774 0.18597236]\n",
      " [0.         0.07747099 0.53260864 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(777)\n",
    "\n",
    "def min_max_scaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "xy = min_max_scaler(xy)\n",
    "print(xy)\n",
    "x_data = xy[:, :-1]\n",
    "y_data = xy[:, [-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost: 0.76073635 \n",
      "Prediction:\n",
      " [[-0.07451153]\n",
      " [ 1.232007  ]\n",
      " [ 0.97038275]\n",
      " [ 0.7682996 ]\n",
      " [ 0.7102202 ]\n",
      " [ 0.86936104]\n",
      " [ 1.1086953 ]\n",
      " [ 1.8699738 ]]\n",
      "20 Cost: 0.7605395 \n",
      "Prediction:\n",
      " [[-0.07467723]\n",
      " [ 1.2317393 ]\n",
      " [ 0.9701561 ]\n",
      " [ 0.7681183 ]\n",
      " [ 0.7100203 ]\n",
      " [ 0.8691604 ]\n",
      " [ 1.1085024 ]\n",
      " [ 1.8697493 ]]\n",
      "40 Cost: 0.76034266 \n",
      "Prediction:\n",
      " [[-0.07484293]\n",
      " [ 1.2314718 ]\n",
      " [ 0.96992946]\n",
      " [ 0.76793706]\n",
      " [ 0.7098204 ]\n",
      " [ 0.8689598 ]\n",
      " [ 1.1083095 ]\n",
      " [ 1.8695247 ]]\n",
      "60 Cost: 0.76014733 \n",
      "Prediction:\n",
      " [[-0.07500708]\n",
      " [ 1.2312056 ]\n",
      " [ 0.96970433]\n",
      " [ 0.7677574 ]\n",
      " [ 0.709622  ]\n",
      " [ 0.86876065]\n",
      " [ 1.1081183 ]\n",
      " [ 1.8693018 ]]\n",
      "80 Cost: 0.75995266 \n",
      "Prediction:\n",
      " [[-0.0751704 ]\n",
      " [ 1.2309403 ]\n",
      " [ 0.96948004]\n",
      " [ 0.7675785 ]\n",
      " [ 0.70942444]\n",
      " [ 0.86856246]\n",
      " [ 1.1079278 ]\n",
      " [ 1.8690796 ]]\n",
      "100 Cost: 0.7597581 \n",
      "Prediction:\n",
      " [[-0.07533371]\n",
      " [ 1.230675  ]\n",
      " [ 0.9692559 ]\n",
      " [ 0.76739967]\n",
      " [ 0.70922685]\n",
      " [ 0.8683642 ]\n",
      " [ 1.1077373 ]\n",
      " [ 1.8688574 ]]\n",
      "120 Cost: 0.7595638 \n",
      "Prediction:\n",
      " [[-0.07549703]\n",
      " [ 1.2304097 ]\n",
      " [ 0.9690316 ]\n",
      " [ 0.7672208 ]\n",
      " [ 0.7090293 ]\n",
      " [ 0.86816597]\n",
      " [ 1.1075469 ]\n",
      " [ 1.8686353 ]]\n",
      "140 Cost: 0.7593694 \n",
      "Prediction:\n",
      " [[-0.07566035]\n",
      " [ 1.2301445 ]\n",
      " [ 0.96880734]\n",
      " [ 0.76704186]\n",
      " [ 0.7088317 ]\n",
      " [ 0.8679677 ]\n",
      " [ 1.1073564 ]\n",
      " [ 1.8684132 ]]\n",
      "160 Cost: 0.7591752 \n",
      "Prediction:\n",
      " [[-0.07582366]\n",
      " [ 1.2298791 ]\n",
      " [ 0.968583  ]\n",
      " [ 0.76686305]\n",
      " [ 0.70863426]\n",
      " [ 0.8677695 ]\n",
      " [ 1.1071659 ]\n",
      " [ 1.868191  ]]\n",
      "180 Cost: 0.758981 \n",
      "Prediction:\n",
      " [[-0.07598698]\n",
      " [ 1.229614  ]\n",
      " [ 0.96835876]\n",
      " [ 0.7666842 ]\n",
      " [ 0.7084367 ]\n",
      " [ 0.86757123]\n",
      " [ 1.1069756 ]\n",
      " [ 1.8679688 ]]\n",
      "200 Cost: 0.7587869 \n",
      "Prediction:\n",
      " [[-0.0761503 ]\n",
      " [ 1.2293487 ]\n",
      " [ 0.96813446]\n",
      " [ 0.7665053 ]\n",
      " [ 0.70823914]\n",
      " [ 0.86737305]\n",
      " [ 1.106785  ]\n",
      " [ 1.8677467 ]]\n",
      "220 Cost: 0.7585929 \n",
      "Prediction:\n",
      " [[-0.07631361]\n",
      " [ 1.2290834 ]\n",
      " [ 0.96791023]\n",
      " [ 0.7663264 ]\n",
      " [ 0.70804155]\n",
      " [ 0.86717474]\n",
      " [ 1.1065946 ]\n",
      " [ 1.8675246 ]]\n",
      "240 Cost: 0.75839895 \n",
      "Prediction:\n",
      " [[-0.07647693]\n",
      " [ 1.228818  ]\n",
      " [ 0.96768594]\n",
      " [ 0.76614755]\n",
      " [ 0.707844  ]\n",
      " [ 0.8669766 ]\n",
      " [ 1.1064041 ]\n",
      " [ 1.8673024 ]]\n",
      "260 Cost: 0.7582052 \n",
      "Prediction:\n",
      " [[-0.07664025]\n",
      " [ 1.2285528 ]\n",
      " [ 0.9674617 ]\n",
      " [ 0.7659687 ]\n",
      " [ 0.7076465 ]\n",
      " [ 0.8667784 ]\n",
      " [ 1.1062137 ]\n",
      " [ 1.8670802 ]]\n",
      "280 Cost: 0.7580115 \n",
      "Prediction:\n",
      " [[-0.07680357]\n",
      " [ 1.2282877 ]\n",
      " [ 0.9672375 ]\n",
      " [ 0.76578987]\n",
      " [ 0.70744896]\n",
      " [ 0.8665801 ]\n",
      " [ 1.1060232 ]\n",
      " [ 1.8668582 ]]\n",
      "300 Cost: 0.75781846 \n",
      "Prediction:\n",
      " [[-0.07696688]\n",
      " [ 1.2280235 ]\n",
      " [ 0.9670139 ]\n",
      " [ 0.7656111 ]\n",
      " [ 0.7072518 ]\n",
      " [ 0.86638224]\n",
      " [ 1.105833  ]\n",
      " [ 1.8666368 ]]\n",
      "320 Cost: 0.7576254 \n",
      "Prediction:\n",
      " [[-0.0771302 ]\n",
      " [ 1.2277595 ]\n",
      " [ 0.9667903 ]\n",
      " [ 0.76543236]\n",
      " [ 0.7070546 ]\n",
      " [ 0.8661845 ]\n",
      " [ 1.1056428 ]\n",
      " [ 1.8664151 ]]\n",
      "340 Cost: 0.7574326 \n",
      "Prediction:\n",
      " [[-0.07729352]\n",
      " [ 1.2274954 ]\n",
      " [ 0.96656674]\n",
      " [ 0.7652536 ]\n",
      " [ 0.7068573 ]\n",
      " [ 0.8659865 ]\n",
      " [ 1.1054525 ]\n",
      " [ 1.8661937 ]]\n",
      "360 Cost: 0.75723976 \n",
      "Prediction:\n",
      " [[-0.07745683]\n",
      " [ 1.2272313 ]\n",
      " [ 0.96634316]\n",
      " [ 0.7650749 ]\n",
      " [ 0.70666015]\n",
      " [ 0.8657887 ]\n",
      " [ 1.1052624 ]\n",
      " [ 1.8659722 ]]\n",
      "380 Cost: 0.757047 \n",
      "Prediction:\n",
      " [[-0.07762015]\n",
      " [ 1.2269672 ]\n",
      " [ 0.96611965]\n",
      " [ 0.76489615]\n",
      " [ 0.706463  ]\n",
      " [ 0.8655909 ]\n",
      " [ 1.1050721 ]\n",
      " [ 1.8657506 ]]\n",
      "400 Cost: 0.75685436 \n",
      "Prediction:\n",
      " [[-0.07778347]\n",
      " [ 1.2267032 ]\n",
      " [ 0.965896  ]\n",
      " [ 0.7647174 ]\n",
      " [ 0.7062658 ]\n",
      " [ 0.865393  ]\n",
      " [ 1.104882  ]\n",
      " [ 1.8655291 ]]\n",
      "420 Cost: 0.75666183 \n",
      "Prediction:\n",
      " [[-0.07794678]\n",
      " [ 1.226439  ]\n",
      " [ 0.96567243]\n",
      " [ 0.76453865]\n",
      " [ 0.70606863]\n",
      " [ 0.86519516]\n",
      " [ 1.1046917 ]\n",
      " [ 1.8653076 ]]\n",
      "440 Cost: 0.75646937 \n",
      "Prediction:\n",
      " [[-0.0781101 ]\n",
      " [ 1.226175  ]\n",
      " [ 0.9654489 ]\n",
      " [ 0.76435995]\n",
      " [ 0.70587146]\n",
      " [ 0.86499727]\n",
      " [ 1.1045015 ]\n",
      " [ 1.8650861 ]]\n",
      "460 Cost: 0.756277 \n",
      "Prediction:\n",
      " [[-0.07827342]\n",
      " [ 1.2259109 ]\n",
      " [ 0.96522534]\n",
      " [ 0.7641812 ]\n",
      " [ 0.70567423]\n",
      " [ 0.8647994 ]\n",
      " [ 1.1043112 ]\n",
      " [ 1.8648646 ]]\n",
      "480 Cost: 0.75608474 \n",
      "Prediction:\n",
      " [[-0.07843673]\n",
      " [ 1.2256467 ]\n",
      " [ 0.9650017 ]\n",
      " [ 0.76400244]\n",
      " [ 0.7054771 ]\n",
      " [ 0.8646016 ]\n",
      " [ 1.1041211 ]\n",
      " [ 1.8646431 ]]\n",
      "500 Cost: 0.7558925 \n",
      "Prediction:\n",
      " [[-0.07860005]\n",
      " [ 1.2253828 ]\n",
      " [ 0.9647781 ]\n",
      " [ 0.7638237 ]\n",
      " [ 0.7052799 ]\n",
      " [ 0.8644036 ]\n",
      " [ 1.1039308 ]\n",
      " [ 1.8644216 ]]\n",
      "520 Cost: 0.75570035 \n",
      "Prediction:\n",
      " [[-0.07876337]\n",
      " [ 1.2251186 ]\n",
      " [ 0.9645546 ]\n",
      " [ 0.76364493]\n",
      " [ 0.70508265]\n",
      " [ 0.86420584]\n",
      " [ 1.1037407 ]\n",
      " [ 1.8642    ]]\n",
      "540 Cost: 0.7555083 \n",
      "Prediction:\n",
      " [[-0.07892668]\n",
      " [ 1.2248546 ]\n",
      " [ 0.9643309 ]\n",
      " [ 0.76346624]\n",
      " [ 0.7048855 ]\n",
      " [ 0.86400795]\n",
      " [ 1.1035504 ]\n",
      " [ 1.8639785 ]]\n",
      "560 Cost: 0.7553164 \n",
      "Prediction:\n",
      " [[-0.07909   ]\n",
      " [ 1.2245904 ]\n",
      " [ 0.96410745]\n",
      " [ 0.76328754]\n",
      " [ 0.7046883 ]\n",
      " [ 0.86381006]\n",
      " [ 1.1033602 ]\n",
      " [ 1.863757  ]]\n",
      "580 Cost: 0.75512457 \n",
      "Prediction:\n",
      " [[-0.0792526 ]\n",
      " [ 1.2243268 ]\n",
      " [ 0.96388423]\n",
      " [ 0.76310897]\n",
      " [ 0.7044915 ]\n",
      " [ 0.86361253]\n",
      " [ 1.10317   ]\n",
      " [ 1.8635356 ]]\n",
      "600 Cost: 0.75493264 \n",
      "Prediction:\n",
      " [[-0.07941473]\n",
      " [ 1.2240636 ]\n",
      " [ 0.96366125]\n",
      " [ 0.76293063]\n",
      " [ 0.7042948 ]\n",
      " [ 0.86341524]\n",
      " [ 1.1029798 ]\n",
      " [ 1.8633142 ]]\n",
      "620 Cost: 0.75474083 \n",
      "Prediction:\n",
      " [[-0.07957685]\n",
      " [ 1.2238004 ]\n",
      " [ 0.9634383 ]\n",
      " [ 0.76275223]\n",
      " [ 0.7040981 ]\n",
      " [ 0.86321783]\n",
      " [ 1.1027895 ]\n",
      " [ 1.8630927 ]]\n",
      "640 Cost: 0.7545504 \n",
      "Prediction:\n",
      " [[-0.07973766]\n",
      " [ 1.2235384 ]\n",
      " [ 0.96321666]\n",
      " [ 0.76257527]\n",
      " [ 0.7039027 ]\n",
      " [ 0.86302185]\n",
      " [ 1.1026007 ]\n",
      " [ 1.8628726 ]]\n",
      "660 Cost: 0.7543608 \n",
      "Prediction:\n",
      " [[-0.0798974 ]\n",
      " [ 1.2232776 ]\n",
      " [ 0.96299595]\n",
      " [ 0.76239926]\n",
      " [ 0.70370835]\n",
      " [ 0.8628268 ]\n",
      " [ 1.1024128 ]\n",
      " [ 1.8626535 ]]\n",
      "680 Cost: 0.7541714 \n",
      "Prediction:\n",
      " [[-0.08005643]\n",
      " [ 1.2230172 ]\n",
      " [ 0.9627758 ]\n",
      " [ 0.7622236 ]\n",
      " [ 0.7035145 ]\n",
      " [ 0.86263216]\n",
      " [ 1.1022252 ]\n",
      " [ 1.8624345 ]]\n",
      "700 Cost: 0.75398207 \n",
      "Prediction:\n",
      " [[-0.08021379]\n",
      " [ 1.222758  ]\n",
      " [ 0.9625567 ]\n",
      " [ 0.76204866]\n",
      " [ 0.70332164]\n",
      " [ 0.86243844]\n",
      " [ 1.1020379 ]\n",
      " [ 1.8622155 ]]\n",
      "720 Cost: 0.7537927 \n",
      "Prediction:\n",
      " [[-0.08037114]\n",
      " [ 1.222499  ]\n",
      " [ 0.96233755]\n",
      " [ 0.76187384]\n",
      " [ 0.70312876]\n",
      " [ 0.8622446 ]\n",
      " [ 1.1018505 ]\n",
      " [ 1.8619964 ]]\n",
      "740 Cost: 0.7536035 \n",
      "Prediction:\n",
      " [[-0.0805285]\n",
      " [ 1.2222397]\n",
      " [ 0.9621183]\n",
      " [ 0.7616989]\n",
      " [ 0.7029358]\n",
      " [ 0.8620508]\n",
      " [ 1.1016632]\n",
      " [ 1.8617774]]\n",
      "760 Cost: 0.7534145 \n",
      "Prediction:\n",
      " [[-0.08068585]\n",
      " [ 1.2219807 ]\n",
      " [ 0.9618993 ]\n",
      " [ 0.761524  ]\n",
      " [ 0.70274293]\n",
      " [ 0.86185694]\n",
      " [ 1.101476  ]\n",
      " [ 1.8615584 ]]\n",
      "780 Cost: 0.7532254 \n",
      "Prediction:\n",
      " [[-0.08084321]\n",
      " [ 1.2217215 ]\n",
      " [ 0.9616802 ]\n",
      " [ 0.7613492 ]\n",
      " [ 0.70255005]\n",
      " [ 0.8616631 ]\n",
      " [ 1.1012886 ]\n",
      " [ 1.8613393 ]]\n",
      "800 Cost: 0.75303644 \n",
      "Prediction:\n",
      " [[-0.08100057]\n",
      " [ 1.2214624 ]\n",
      " [ 0.9614609 ]\n",
      " [ 0.76117426]\n",
      " [ 0.70235705]\n",
      " [ 0.8614693 ]\n",
      " [ 1.1011013 ]\n",
      " [ 1.8611203 ]]\n",
      "820 Cost: 0.75284755 \n",
      "Prediction:\n",
      " [[-0.08115792]\n",
      " [ 1.2212032 ]\n",
      " [ 0.96124184]\n",
      " [ 0.7609994 ]\n",
      " [ 0.7021642 ]\n",
      " [ 0.86127555]\n",
      " [ 1.100914  ]\n",
      " [ 1.8609012 ]]\n",
      "840 Cost: 0.75265884 \n",
      "Prediction:\n",
      " [[-0.08131528]\n",
      " [ 1.2209442 ]\n",
      " [ 0.9610227 ]\n",
      " [ 0.76082444]\n",
      " [ 0.7019713 ]\n",
      " [ 0.8610816 ]\n",
      " [ 1.1007267 ]\n",
      " [ 1.8606822 ]]\n",
      "860 Cost: 0.75247014 \n",
      "Prediction:\n",
      " [[-0.08147264]\n",
      " [ 1.220685  ]\n",
      " [ 0.9608035 ]\n",
      " [ 0.7606496 ]\n",
      " [ 0.7017784 ]\n",
      " [ 0.8608878 ]\n",
      " [ 1.1005394 ]\n",
      " [ 1.8604631 ]]\n",
      "880 Cost: 0.75228155 \n",
      "Prediction:\n",
      " [[-0.08162999]\n",
      " [ 1.2204258 ]\n",
      " [ 0.9605843 ]\n",
      " [ 0.76047474]\n",
      " [ 0.70158553]\n",
      " [ 0.86069405]\n",
      " [ 1.100352  ]\n",
      " [ 1.8602442 ]]\n",
      "900 Cost: 0.75209296 \n",
      "Prediction:\n",
      " [[-0.08178735]\n",
      " [ 1.2201666 ]\n",
      " [ 0.96036524]\n",
      " [ 0.76029986]\n",
      " [ 0.7013926 ]\n",
      " [ 0.8605002 ]\n",
      " [ 1.1001648 ]\n",
      " [ 1.8600252 ]]\n",
      "920 Cost: 0.7519045 \n",
      "Prediction:\n",
      " [[-0.0819447 ]\n",
      " [ 1.2199075 ]\n",
      " [ 0.96014607]\n",
      " [ 0.760125  ]\n",
      " [ 0.7011997 ]\n",
      " [ 0.8603064 ]\n",
      " [ 1.0999775 ]\n",
      " [ 1.8598061 ]]\n",
      "940 Cost: 0.75171614 \n",
      "Prediction:\n",
      " [[-0.08210206]\n",
      " [ 1.2196484 ]\n",
      " [ 0.95992684]\n",
      " [ 0.75995004]\n",
      " [ 0.70100677]\n",
      " [ 0.86011255]\n",
      " [ 1.0997902 ]\n",
      " [ 1.8595871 ]]\n",
      "960 Cost: 0.7515279 \n",
      "Prediction:\n",
      " [[-0.08225942]\n",
      " [ 1.2193892 ]\n",
      " [ 0.95970774]\n",
      " [ 0.75977516]\n",
      " [ 0.7008139 ]\n",
      " [ 0.8599187 ]\n",
      " [ 1.0996028 ]\n",
      " [ 1.8593681 ]]\n",
      "980 Cost: 0.75133985 \n",
      "Prediction:\n",
      " [[-0.08241677]\n",
      " [ 1.2191303 ]\n",
      " [ 0.9594888 ]\n",
      " [ 0.7596004 ]\n",
      " [ 0.7006211 ]\n",
      " [ 0.85972506]\n",
      " [ 1.0994157 ]\n",
      " [ 1.8591492 ]]\n",
      "1000 Cost: 0.7511523 \n",
      "Prediction:\n",
      " [[-0.08257413]\n",
      " [ 1.2188725 ]\n",
      " [ 0.95927036]\n",
      " [ 0.7594256 ]\n",
      " [ 0.7004286 ]\n",
      " [ 0.85953164]\n",
      " [ 1.0992286 ]\n",
      " [ 1.8589308 ]]\n",
      "1020 Cost: 0.7509648 \n",
      "Prediction:\n",
      " [[-0.08273149]\n",
      " [ 1.2186145 ]\n",
      " [ 0.9590519 ]\n",
      " [ 0.7592509 ]\n",
      " [ 0.700236  ]\n",
      " [ 0.85933816]\n",
      " [ 1.0990415 ]\n",
      " [ 1.8587124 ]]\n",
      "1040 Cost: 0.7507774 \n",
      "Prediction:\n",
      " [[-0.08288884]\n",
      " [ 1.2183566 ]\n",
      " [ 0.9588334 ]\n",
      " [ 0.75907606]\n",
      " [ 0.70004344]\n",
      " [ 0.8591447 ]\n",
      " [ 1.0988544 ]\n",
      " [ 1.8584939 ]]\n",
      "1060 Cost: 0.75059015 \n",
      "Prediction:\n",
      " [[-0.0830462 ]\n",
      " [ 1.2180985 ]\n",
      " [ 0.95861495]\n",
      " [ 0.75890136]\n",
      " [ 0.6998509 ]\n",
      " [ 0.85895133]\n",
      " [ 1.0986674 ]\n",
      " [ 1.8582755 ]]\n",
      "1080 Cost: 0.7504029 \n",
      "Prediction:\n",
      " [[-0.08320355]\n",
      " [ 1.2178407 ]\n",
      " [ 0.9583965 ]\n",
      " [ 0.7587266 ]\n",
      " [ 0.6996584 ]\n",
      " [ 0.85875773]\n",
      " [ 1.0984803 ]\n",
      " [ 1.8580571 ]]\n",
      "1100 Cost: 0.75021577 \n",
      "Prediction:\n",
      " [[-0.08336091]\n",
      " [ 1.2175827 ]\n",
      " [ 0.958178  ]\n",
      " [ 0.75855184]\n",
      " [ 0.6994659 ]\n",
      " [ 0.8585644 ]\n",
      " [ 1.0982933 ]\n",
      " [ 1.8578387 ]]\n",
      "1120 Cost: 0.7500288 \n",
      "Prediction:\n",
      " [[-0.08351803]\n",
      " [ 1.2173249 ]\n",
      " [ 0.9579598 ]\n",
      " [ 0.75837713]\n",
      " [ 0.69927347]\n",
      " [ 0.858371  ]\n",
      " [ 1.0981063 ]\n",
      " [ 1.8576204 ]]\n",
      "1140 Cost: 0.74984175 \n",
      "Prediction:\n",
      " [[-0.083673  ]\n",
      " [ 1.2170687 ]\n",
      " [ 0.9577426 ]\n",
      " [ 0.75820315]\n",
      " [ 0.69908214]\n",
      " [ 0.8581788 ]\n",
      " [ 1.0979195 ]\n",
      " [ 1.857402  ]]\n",
      "1160 Cost: 0.7496547 \n",
      "Prediction:\n",
      " [[-0.08382797]\n",
      " [ 1.2168124 ]\n",
      " [ 0.9575254 ]\n",
      " [ 0.7580293 ]\n",
      " [ 0.6988908 ]\n",
      " [ 0.85798645]\n",
      " [ 1.0977327 ]\n",
      " [ 1.8571835 ]]\n",
      "1180 Cost: 0.74946785 \n",
      "Prediction:\n",
      " [[-0.08398294]\n",
      " [ 1.2165561 ]\n",
      " [ 0.9573082 ]\n",
      " [ 0.7578553 ]\n",
      " [ 0.6986995 ]\n",
      " [ 0.8577943 ]\n",
      " [ 1.0975459 ]\n",
      " [ 1.8569651 ]]\n",
      "1200 Cost: 0.74928105 \n",
      "Prediction:\n",
      " [[-0.08413792]\n",
      " [ 1.2162998 ]\n",
      " [ 0.95709103]\n",
      " [ 0.75768137]\n",
      " [ 0.6985082 ]\n",
      " [ 0.857602  ]\n",
      " [ 1.0973591 ]\n",
      " [ 1.8567467 ]]\n",
      "1220 Cost: 0.74909437 \n",
      "Prediction:\n",
      " [[-0.08429289]\n",
      " [ 1.2160435 ]\n",
      " [ 0.9568739 ]\n",
      " [ 0.75750744]\n",
      " [ 0.6983169 ]\n",
      " [ 0.8574098 ]\n",
      " [ 1.0971723 ]\n",
      " [ 1.8565283 ]]\n",
      "1240 Cost: 0.7489092 \n",
      "Prediction:\n",
      " [[-0.08444607]\n",
      " [ 1.2157891 ]\n",
      " [ 0.9566585 ]\n",
      " [ 0.75733525]\n",
      " [ 0.69812727]\n",
      " [ 0.8572192 ]\n",
      " [ 1.0969872 ]\n",
      " [ 1.8563117 ]]\n",
      "1260 Cost: 0.74872464 \n",
      "Prediction:\n",
      " [[-0.08459866]\n",
      " [ 1.2155352 ]\n",
      " [ 0.9564437 ]\n",
      " [ 0.75716364]\n",
      " [ 0.69793844]\n",
      " [ 0.8570294 ]\n",
      " [ 1.096803  ]\n",
      " [ 1.8560957 ]]\n",
      "1280 Cost: 0.7485401 \n",
      "Prediction:\n",
      " [[-0.08475018]\n",
      " [ 1.215282  ]\n",
      " [ 0.95622945]\n",
      " [ 0.7569925 ]\n",
      " [ 0.69775   ]\n",
      " [ 0.85683995]\n",
      " [ 1.0966185 ]\n",
      " [ 1.8558798 ]]\n",
      "1300 Cost: 0.7483556 \n",
      "Prediction:\n",
      " [[-0.08490157]\n",
      " [ 1.215029  ]\n",
      " [ 0.95601535]\n",
      " [ 0.7568213 ]\n",
      " [ 0.6975615 ]\n",
      " [ 0.8566506 ]\n",
      " [ 1.0964341 ]\n",
      " [ 1.8556638 ]]\n",
      "1320 Cost: 0.7481712 \n",
      "Prediction:\n",
      " [[-0.08505297]\n",
      " [ 1.2147759 ]\n",
      " [ 0.9558012 ]\n",
      " [ 0.7566501 ]\n",
      " [ 0.6973731 ]\n",
      " [ 0.85646117]\n",
      " [ 1.0962497 ]\n",
      " [ 1.8554479 ]]\n",
      "1340 Cost: 0.7479869 \n",
      "Prediction:\n",
      " [[-0.08520436]\n",
      " [ 1.2145228 ]\n",
      " [ 0.9555869 ]\n",
      " [ 0.75647885]\n",
      " [ 0.6971847 ]\n",
      " [ 0.85627186]\n",
      " [ 1.0960653 ]\n",
      " [ 1.855232  ]]\n",
      "1360 Cost: 0.7478026 \n",
      "Prediction:\n",
      " [[-0.08535576]\n",
      " [ 1.2142698 ]\n",
      " [ 0.9553727 ]\n",
      " [ 0.7563077 ]\n",
      " [ 0.6969962 ]\n",
      " [ 0.85608244]\n",
      " [ 1.0958809 ]\n",
      " [ 1.855016  ]]\n",
      "1380 Cost: 0.74761856 \n",
      "Prediction:\n",
      " [[-0.08550715]\n",
      " [ 1.2140167 ]\n",
      " [ 0.9551586 ]\n",
      " [ 0.75613654]\n",
      " [ 0.69680786]\n",
      " [ 0.85589314]\n",
      " [ 1.0956964 ]\n",
      " [ 1.8548002 ]]\n",
      "1400 Cost: 0.74743444 \n",
      "Prediction:\n",
      " [[-0.08565855]\n",
      " [ 1.2137637 ]\n",
      " [ 0.9549445 ]\n",
      " [ 0.75596535]\n",
      " [ 0.6966194 ]\n",
      " [ 0.8557037 ]\n",
      " [ 1.095512  ]\n",
      " [ 1.8545842 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1420 Cost: 0.7472504 \n",
      "Prediction:\n",
      " [[-0.08580995]\n",
      " [ 1.2135106 ]\n",
      " [ 0.9547302 ]\n",
      " [ 0.75579417]\n",
      " [ 0.69643104]\n",
      " [ 0.8555143 ]\n",
      " [ 1.0953276 ]\n",
      " [ 1.8543682 ]]\n",
      "1440 Cost: 0.74706656 \n",
      "Prediction:\n",
      " [[-0.08596134]\n",
      " [ 1.2132576 ]\n",
      " [ 0.95451605]\n",
      " [ 0.755623  ]\n",
      " [ 0.6962426 ]\n",
      " [ 0.855325  ]\n",
      " [ 1.0951433 ]\n",
      " [ 1.8541524 ]]\n",
      "1460 Cost: 0.7468827 \n",
      "Prediction:\n",
      " [[-0.08611274]\n",
      " [ 1.2130046 ]\n",
      " [ 0.95430183]\n",
      " [ 0.75545174]\n",
      " [ 0.69605416]\n",
      " [ 0.8551356 ]\n",
      " [ 1.0949589 ]\n",
      " [ 1.8539364 ]]\n",
      "1480 Cost: 0.746699 \n",
      "Prediction:\n",
      " [[-0.08626413]\n",
      " [ 1.2127514 ]\n",
      " [ 0.9540877 ]\n",
      " [ 0.7552806 ]\n",
      " [ 0.69586575]\n",
      " [ 0.85494626]\n",
      " [ 1.0947745 ]\n",
      " [ 1.8537205 ]]\n",
      "1500 Cost: 0.7465153 \n",
      "Prediction:\n",
      " [[-0.08641553]\n",
      " [ 1.2124984 ]\n",
      " [ 0.9538735 ]\n",
      " [ 0.75510937]\n",
      " [ 0.6956773 ]\n",
      " [ 0.85475683]\n",
      " [ 1.0945901 ]\n",
      " [ 1.8535047 ]]\n",
      "1520 Cost: 0.7463317 \n",
      "Prediction:\n",
      " [[-0.08656693]\n",
      " [ 1.2122453 ]\n",
      " [ 0.95365936]\n",
      " [ 0.75493824]\n",
      " [ 0.69548887]\n",
      " [ 0.8545675 ]\n",
      " [ 1.0944057 ]\n",
      " [ 1.8532887 ]]\n",
      "1540 Cost: 0.7461481 \n",
      "Prediction:\n",
      " [[-0.08671832]\n",
      " [ 1.2119923 ]\n",
      " [ 0.95344514]\n",
      " [ 0.75476706]\n",
      " [ 0.69530046]\n",
      " [ 0.8543781 ]\n",
      " [ 1.0942212 ]\n",
      " [ 1.8530728 ]]\n",
      "1560 Cost: 0.74596465 \n",
      "Prediction:\n",
      " [[-0.08686972]\n",
      " [ 1.2117393 ]\n",
      " [ 0.95323086]\n",
      " [ 0.7545958 ]\n",
      " [ 0.69511205]\n",
      " [ 0.85418874]\n",
      " [ 1.0940368 ]\n",
      " [ 1.8528569 ]]\n",
      "1580 Cost: 0.7457813 \n",
      "Prediction:\n",
      " [[-0.08702111]\n",
      " [ 1.2114862 ]\n",
      " [ 0.9530168 ]\n",
      " [ 0.75442463]\n",
      " [ 0.69492364]\n",
      " [ 0.8539994 ]\n",
      " [ 1.0938525 ]\n",
      " [ 1.8526409 ]]\n",
      "1600 Cost: 0.74559796 \n",
      "Prediction:\n",
      " [[-0.08717251]\n",
      " [ 1.2112331 ]\n",
      " [ 0.95280266]\n",
      " [ 0.7542534 ]\n",
      " [ 0.69473517]\n",
      " [ 0.85381   ]\n",
      " [ 1.093668  ]\n",
      " [ 1.852425  ]]\n",
      "1620 Cost: 0.74541485 \n",
      "Prediction:\n",
      " [[-0.0873239 ]\n",
      " [ 1.2109802 ]\n",
      " [ 0.95258844]\n",
      " [ 0.75408226]\n",
      " [ 0.6945468 ]\n",
      " [ 0.85362065]\n",
      " [ 1.0934837 ]\n",
      " [ 1.8522091 ]]\n",
      "1640 Cost: 0.7452317 \n",
      "Prediction:\n",
      " [[-0.0874753 ]\n",
      " [ 1.210727  ]\n",
      " [ 0.95237416]\n",
      " [ 0.7539111 ]\n",
      " [ 0.69435835]\n",
      " [ 0.8534313 ]\n",
      " [ 1.0932993 ]\n",
      " [ 1.8519931 ]]\n",
      "1660 Cost: 0.74504864 \n",
      "Prediction:\n",
      " [[-0.0876267]\n",
      " [ 1.210474 ]\n",
      " [ 0.95216  ]\n",
      " [ 0.7537399]\n",
      " [ 0.6941699]\n",
      " [ 0.8532419]\n",
      " [ 1.0931149]\n",
      " [ 1.8517772]]\n",
      "1680 Cost: 0.74486566 \n",
      "Prediction:\n",
      " [[-0.08777809]\n",
      " [ 1.210221  ]\n",
      " [ 0.9519459 ]\n",
      " [ 0.7535687 ]\n",
      " [ 0.6939815 ]\n",
      " [ 0.85305256]\n",
      " [ 1.0929304 ]\n",
      " [ 1.8515613 ]]\n",
      "1700 Cost: 0.74468315 \n",
      "Prediction:\n",
      " [[-0.08792949]\n",
      " [ 1.2099688 ]\n",
      " [ 0.95173216]\n",
      " [ 0.7533976 ]\n",
      " [ 0.69379336]\n",
      " [ 0.85286343]\n",
      " [ 1.0927463 ]\n",
      " [ 1.8513458 ]]\n",
      "1720 Cost: 0.7445009 \n",
      "Prediction:\n",
      " [[-0.08808088]\n",
      " [ 1.2097169 ]\n",
      " [ 0.95151865]\n",
      " [ 0.7532265 ]\n",
      " [ 0.6936053 ]\n",
      " [ 0.85267437]\n",
      " [ 1.092562  ]\n",
      " [ 1.8511305 ]]\n",
      "1740 Cost: 0.7443187 \n",
      "Prediction:\n",
      " [[-0.08823228]\n",
      " [ 1.209465  ]\n",
      " [ 0.95130515]\n",
      " [ 0.75305545]\n",
      " [ 0.6934172 ]\n",
      " [ 0.8524854 ]\n",
      " [ 1.0923779 ]\n",
      " [ 1.8509152 ]]\n",
      "1760 Cost: 0.74413663 \n",
      "Prediction:\n",
      " [[-0.08838367]\n",
      " [ 1.2092133 ]\n",
      " [ 0.95109165]\n",
      " [ 0.7528844 ]\n",
      " [ 0.6932292 ]\n",
      " [ 0.8522964 ]\n",
      " [ 1.0921937 ]\n",
      " [ 1.8506999 ]]\n",
      "1780 Cost: 0.74395466 \n",
      "Prediction:\n",
      " [[-0.08853507]\n",
      " [ 1.2089612 ]\n",
      " [ 0.9508782 ]\n",
      " [ 0.7527134 ]\n",
      " [ 0.69304115]\n",
      " [ 0.8521074 ]\n",
      " [ 1.0920095 ]\n",
      " [ 1.8504846 ]]\n",
      "1800 Cost: 0.7437727 \n",
      "Prediction:\n",
      " [[-0.08868647]\n",
      " [ 1.2087095 ]\n",
      " [ 0.9506647 ]\n",
      " [ 0.75254226]\n",
      " [ 0.69285303]\n",
      " [ 0.8519184 ]\n",
      " [ 1.0918254 ]\n",
      " [ 1.8502693 ]]\n",
      "1820 Cost: 0.74359083 \n",
      "Prediction:\n",
      " [[-0.08883786]\n",
      " [ 1.2084577 ]\n",
      " [ 0.95045114]\n",
      " [ 0.75237125]\n",
      " [ 0.692665  ]\n",
      " [ 0.85172945]\n",
      " [ 1.0916412 ]\n",
      " [ 1.850054  ]]\n",
      "1840 Cost: 0.74340963 \n",
      "Prediction:\n",
      " [[-0.08898854]\n",
      " [ 1.2082064 ]\n",
      " [ 0.9502384 ]\n",
      " [ 0.7522009 ]\n",
      " [ 0.6924776 ]\n",
      " [ 0.85154116]\n",
      " [ 1.0914578 ]\n",
      " [ 1.8498394 ]]\n",
      "1860 Cost: 0.7432299 \n",
      "Prediction:\n",
      " [[-0.08913755]\n",
      " [ 1.207957  ]\n",
      " [ 0.95002735]\n",
      " [ 0.75203216]\n",
      " [ 0.692292  ]\n",
      " [ 0.8513546 ]\n",
      " [ 1.091276  ]\n",
      " [ 1.8496265 ]]\n",
      "1880 Cost: 0.7430502 \n",
      "Prediction:\n",
      " [[-0.08928633]\n",
      " [ 1.2077076 ]\n",
      " [ 0.94981635]\n",
      " [ 0.75186366]\n",
      " [ 0.6921064 ]\n",
      " [ 0.85116804]\n",
      " [ 1.0910943 ]\n",
      " [ 1.8494136 ]]\n",
      "1900 Cost: 0.7428706 \n",
      "Prediction:\n",
      " [[-0.08943295]\n",
      " [ 1.2074599 ]\n",
      " [ 0.94960666]\n",
      " [ 0.7516961 ]\n",
      " [ 0.6919222 ]\n",
      " [ 0.8509826 ]\n",
      " [ 1.090913  ]\n",
      " [ 1.8492007 ]]\n",
      "1920 Cost: 0.74269116 \n",
      "Prediction:\n",
      " [[-0.08957958]\n",
      " [ 1.2072122 ]\n",
      " [ 0.9493971 ]\n",
      " [ 0.7515285 ]\n",
      " [ 0.6917379 ]\n",
      " [ 0.85079706]\n",
      " [ 1.0907319 ]\n",
      " [ 1.8489878 ]]\n",
      "1940 Cost: 0.74251175 \n",
      "Prediction:\n",
      " [[-0.08972621]\n",
      " [ 1.2069644 ]\n",
      " [ 0.9491874 ]\n",
      " [ 0.75136095]\n",
      " [ 0.6915536 ]\n",
      " [ 0.8506117 ]\n",
      " [ 1.0905505 ]\n",
      " [ 1.8487749 ]]\n",
      "1960 Cost: 0.74233234 \n",
      "Prediction:\n",
      " [[-0.08987284]\n",
      " [ 1.2067167 ]\n",
      " [ 0.9489777 ]\n",
      " [ 0.75119334]\n",
      " [ 0.6913693 ]\n",
      " [ 0.8504262 ]\n",
      " [ 1.0903693 ]\n",
      " [ 1.848562  ]]\n",
      "1980 Cost: 0.74215305 \n",
      "Prediction:\n",
      " [[-0.09001935]\n",
      " [ 1.2064688 ]\n",
      " [ 0.948768  ]\n",
      " [ 0.7510258 ]\n",
      " [ 0.6911851 ]\n",
      " [ 0.85024077]\n",
      " [ 1.090188  ]\n",
      " [ 1.8483491 ]]\n",
      "2000 Cost: 0.7419738 \n",
      "Prediction:\n",
      " [[-0.09016478]\n",
      " [ 1.206222  ]\n",
      " [ 0.94855905]\n",
      " [ 0.7508586 ]\n",
      " [ 0.6910013 ]\n",
      " [ 0.8500558 ]\n",
      " [ 1.0900068 ]\n",
      " [ 1.8481363 ]]\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# model / hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-5).minimize(cost)\n",
    "\n",
    "# Launch the graph\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    if step % 20 == 0:\n",
    "        print(step, \"Cost:\", cost_val, \"\\nPrediction:\\n\", hy_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cost 함수 값이 줄어들지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예제 4. MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001, Cost: 2.555483790\n",
      "Epoch: 0002, Cost: 1.101893025\n",
      "Epoch: 0003, Cost: 0.882419495\n",
      "Epoch: 0004, Cost: 0.773668487\n",
      "Epoch: 0005, Cost: 0.705237583\n",
      "Epoch: 0006, Cost: 0.656483896\n",
      "Epoch: 0007, Cost: 0.619449151\n",
      "Epoch: 0008, Cost: 0.590239947\n",
      "Epoch: 0009, Cost: 0.565300263\n",
      "Epoch: 0010, Cost: 0.545157557\n",
      "Epoch: 0011, Cost: 0.527119241\n",
      "Epoch: 0012, Cost: 0.511464164\n",
      "Epoch: 0013, Cost: 0.498071650\n",
      "Epoch: 0014, Cost: 0.485856056\n",
      "Epoch: 0015, Cost: 0.474693939\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "# 28 x 28 x 1 image 데이터\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28 * 28])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([28 * 28, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "# hypothesis\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Test model\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
    "\n",
    "# parameters\n",
    "num_epochs = 15  # 전체 데이터셋을 15개 잘라서 트레이닝\n",
    "batch_size = 100\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "                      \n",
    "# Launch the graph\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# training cycle\n",
    "for epoch in range(num_epochs):\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size) # 100개씩 불러옴\n",
    "        _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "        avg_cost += cost_val / num_iterations\n",
    "    \n",
    "    print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch+1, avg_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8862\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: [4]\n",
      "Prediction:  [4]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Get one and predict\n",
    "# r = random.randint(0, mnist.test.num_examples - 1)\n",
    "idx = np.random.choice(mnist.test.num_examples, 1)[0]\n",
    "print(\"Label:\", sess.run(tf.argmax(mnist.test.labels[idx:idx+1], 1)))\n",
    "print(\"Prediction: \", sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[idx:idx+1]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(mnist.test.images[idx: idx+1].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
