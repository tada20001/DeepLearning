{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/imjunghee/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "<tf.Variable 'Variable:0' shape=(3, 3, 1, 32) dtype=float32_ref>\n",
      "Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
      "Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
      "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# input placeholders\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28 * 28])\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1])  #img 28 x 28 x 1 (black/white)\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "# L1 ImgIn shape=[?, 28, 28, 1]\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))  # 3 x 3 색깔은 하나 @ 32 filters\n",
    "print(W1)\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding=\"SAME\")  # convolution layers(weight 곱한 형태), 인풋이미지와 같은 사이즈\n",
    "print(L1)\n",
    "L1 = tf.nn.relu(L1)\n",
    "print(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  # 반으로 줄인 사이즈 ?, 14, 14, 1\n",
    "print(L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_1:0' shape=(3, 3, 32, 64) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_1:0' shape=(3, 3, 32, 64) dtype=float32_ref>\n",
      "Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
      "Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# L2 ImgIn shape=(?, 14, 14, 32)\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))  # 3 x 3 32개 색 @ 64 filters\n",
    "print(W2)\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')  # ? 14, 14, 64\n",
    "print(W2)\n",
    "L2 = tf.nn.relu(L2)\n",
    "print(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') # ?, 7, 7, 64\n",
    "print(L2)\n",
    "L2 = tf.reshape(L2, [-1, 7 * 7 * 64])  # for fully connected layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected layer(FC, Dense layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3136\n"
     ]
    }
   ],
   "source": [
    "# Final FC 7 x 7 x 64 inputs -> 10 inputs\n",
    "W3_size = L2.shape[1]\n",
    "print(W3_size)\n",
    "W3 = tf.get_variable(\"W3\", shape=[W3_size, 10], initializer=tf.contrib.layers.xavier_initializer())  # weight 초기값 xavier 방법 설정\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b\n",
    "\n",
    "# define cost/loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Cost: 0.147327252\n",
      "Epoch: 0002 Cost: 0.051119460\n",
      "Epoch: 0003 Cost: 0.038677091\n",
      "Epoch: 0004 Cost: 0.030593015\n",
      "Epoch: 0005 Cost: 0.028235704\n",
      "Epoch: 0006 Cost: 0.024724376\n",
      "Epoch: 0007 Cost: 0.024477969\n",
      "Epoch: 0008 Cost: 0.020663796\n",
      "Epoch: 0009 Cost: 0.020871267\n",
      "Epoch: 0010 Cost: 0.024781698\n",
      "Epoch: 0011 Cost: 0.018642781\n",
      "Epoch: 0012 Cost: 0.016119871\n",
      "Epoch: 0013 Cost: 0.016837329\n",
      "Epoch: 0014 Cost: 0.018158276\n",
      "Epoch: 0015 Cost: 0.018436751\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size = 100\n",
    "num_epochs = 15\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    avg_cost = 0\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "        avg_cost += c / num_iterations\n",
    "    \n",
    "    print(\"Epoch:\", \"%04d\" % (epoch + 1), \"Cost:\", \"%.9f\" % (avg_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.985\n"
     ]
    }
   ],
   "source": [
    "# Accuray computation\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, axis=1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, dtype=tf.float32))\n",
    "\n",
    "print(\"Accuracy:\", sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: [3] Prediction: [3]\n"
     ]
    }
   ],
   "source": [
    "# Get one and predict\n",
    "idx = np.random.choice(mnist.test.num_examples, 1)[0]\n",
    "label = sess.run(tf.argmax(mnist.test.labels[idx:idx+1], 1))\n",
    "predict = sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[idx:idx+1]})\n",
    "print(\"Label:\", label, \"Prediction:\", predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.test.images[idx:idx+1].reshape(28, 28), cmap='Greys', interpolation=\"nearest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
