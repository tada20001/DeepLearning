{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weight 초기값\n",
    "\n",
    "* 초기값을 어떻게 설정할 것인가?  vanishing gradient 문제를 해결하는 방법으로 초기값 설정도 중요하다.$$$$\n",
    "\n",
    "* ReLU에 같은 값을 주더라도 cost 함수 결과가 다르게 나온다. $$$$\n",
    "\n",
    "* 예를 들어, weight 초기값을 0으로 준다면 앞단계의 gradient가 사라진다. 학습이 전혀 안된다. $$$$\n",
    "\n",
    "* 이 문제를 해결하기 위하여 **Restricted Boatman Machine(RBM)** 을 사용하여 weight 초기값을 설정하는 방법이 제안되었다.$$$$\n",
    "\n",
    "    * 하나의 레이어에서 input x 값을 넣어서 w, b를 곱하여 만든 값을 다시 backward로 b, w를 곱하여 다시 input을 만든다.(recreate input) 여기에서  실제 input 값과 재생산된 input 값의 차가 최소가 되는 w, b 값을 찾는다.$$$$\n",
    "    * 이러한 과정을 PRE-TRAINING 또는 FINE TUNING이라고 부른다. 즉, weight를 학습시키는 과정이다.$$$$ \n",
    "\n",
    "\n",
    "* 그러나 RBM 방법은 좋으나 복잡한 방법이다. 이에 대하여 새롭게 나온 방법이 굉장히 간단한 초기값을 줘도 상관없다는 연구가 발표되었다.$$$$\n",
    "    * Xavier initialization\n",
    "    * He's initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier / He initialization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Xavier\n",
    "W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in)\n",
    "\n",
    "# He\n",
    "W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
